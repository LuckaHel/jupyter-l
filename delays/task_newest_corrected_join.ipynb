{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118939be-42d4-4a27-b4df-758358c8e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_date, concat, lit\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/mate/.local/lib/python3.10/site-packages/pyspark/\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628aa5bc-3116-432b-836b-b7180bf82f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/07 23:44:06 WARN Utils: Your hostname, ces-shrd-1 resolves to a loopback address: 127.0.1.1; using 192.168.1.25 instead (on interface wlp0s20f3)\n",
      "24/05/07 23:44:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/07 23:44:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"My Spark Application\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01556bb0-76d9-41a3-8586-e6275b034fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, dayofweek,to_date, month, count, avg\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number,   sum, when\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path_flightdelay = \"./full_data_flightdelay.csv\"  # Replace with the path to your CSV file\n",
    "\n",
    "\n",
    "df_flightdelay = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(csv_file_path_flightdelay)\n",
    "\n",
    "\n",
    "# Read the CSV file using the manually defined schema\n",
    "csv_file_path_weather = \"./airport_weather_2019.csv\"  # Replace with your file path\n",
    "df_weather = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(csv_file_path_weather)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab71331e-dd5a-4fe4-96d8-971c711c5836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather.select('DATE').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ea6c03-0380-4397-a6b4-c138cc21647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Data cleanup and preparation\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c09dbf2-39ce-42c3-9e35-35ffc9c47fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/07 23:44:11 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# create new column for month and day_of_week values derived from date\n",
    "df_day_column = df_weather.withColumn(\"DATE_NEW\", to_date(col(\"DATE\"), \"M/d/yyyy\"))\n",
    "df_day_column = df_day_column.withColumn(\"DATE_NEW\", coalesce(df_day_column[\"DATE_NEW\"], to_date(df_day_column[\"DATE\"], 'yyyy-MM-dd')))\n",
    "\n",
    "df_day_column = df_day_column.withColumn(\"DAY_OF_WEEK\", dayofweek(col(\"DATE_NEW\").alias(\"DAY_OF_WEEK\")))\n",
    "df_day_column = df_day_column.withColumn(\"MONTH\", month(col(\"DATE_NEW\").alias(\"MONTH\")))\n",
    "\n",
    "df_day_column.createOrReplaceTempView(\"table1\")\n",
    "df_select = spark.sql(\"SELECT STATION, NAME,DAY_OF_WEEK,DATE, MONTH, AWND, PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, WDF2 from table1\")\n",
    "#df_select.show(n=5)\n",
    "\n",
    "grouped_df = df_select.groupBy(\"MONTH\", \"NAME\").agg(\n",
    "    avg(\"AWND\").alias(\"AWND\"),\n",
    "    avg(\"PRCP\").alias(\"PRCP\"),\n",
    "    avg(\"SNOW\").alias(\"SNOW\"),\n",
    "    avg(\"SNWD\").alias(\"SNWD\"),\n",
    "    avg(\"TAVG\").alias(\"TAVG\"),\n",
    "    avg(\"TMAX\").alias(\"TMAX\"),\n",
    "    avg(\"TMIN\").alias(\"TMIN\"),\n",
    "    avg(\"WDF2\").alias(\"WDF2\")\n",
    ").orderBy(\"NAME\",\"MONTH\")\n",
    "\n",
    "\n",
    "#grouped_df.show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983d5e23-d23d-49cc-b55b-1427c1e118ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, split, col, lit, monotonically_increasing_id\n",
    "\n",
    "\n",
    "# Normalize joining columns\n",
    "grouped_df = grouped_df.withColumn(\"normalized_name\", lower(col(\"name\")))\n",
    "df_flightdelay = df_flightdelay.withColumn(\"normalized_name\", lower(split(col(\"departing_airport\"), \" \").getItem(0)))\n",
    "\n",
    "# Group by to investigate\n",
    "grouped_df_nn = grouped_df.groupBy(\"normalized_name\").agg(\n",
    "    count('*').alias('count')\n",
    ")\n",
    "\n",
    "grouped_df_name = grouped_df.groupBy(\"NAME\").agg(\n",
    "    count('*').alias('count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66837d93-05eb-4042-bf6b-1159fcb003df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'grouped_df', transforming 'NAME' to lowercase and dropping duplicates based on the 'name' column\n",
    "grouped_df_lower = grouped_df.select(lower(col(\"NAME\")).alias(\"name\")).dropDuplicates(['name'])\n",
    "\n",
    "# For 'df_flightdelay', transforming 'DEPARTING_AIRPORT' to lowercase, casting it to string, and dropping duplicates based on the 'departing_airport' column\n",
    "df_flightdelay_lower = df_flightdelay.select(lower(col(\"DEPARTING_AIRPORT\")).alias(\"departing_airport\")).dropDuplicates(['departing_airport'])\n",
    "\n",
    "\n",
    "\n",
    "#join providing table that contain in the name column all distinct airports \n",
    "#from weather dataset and under departing_flight all distinc airports from delay dataset\n",
    "\n",
    "result_df = df_flightdelay_lower.alias(\"flight\").join(\n",
    "    grouped_df_lower.alias(\"grouped\"),\n",
    "    (col(\"grouped.name\").contains(col(\"flight.departing_airport\"))),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"flight.departing_airport\").alias(\"departing_airport\"),\n",
    "    col(\"grouped.name\").alias(\"name\")\n",
    ")\n",
    "\n",
    "#result_df.show(n=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cad8c016-135e-4c98-83d9-f36c28a72c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modify dataframe such that df_result will contain the airports matched on join \n",
    "#and enhanced results will contain the df of unmatched airports for each dataset\n",
    "\n",
    "# Identifying non-matched entries\n",
    "non_matched_flight = df_flightdelay_lower.alias(\"flight\").join(\n",
    "    result_df.alias(\"result\"),\n",
    "    result_df.departing_airport == df_flightdelay_lower.departing_airport,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "non_matched_grouped = grouped_df_lower.alias(\"grouped\").join(\n",
    "    result_df.alias(\"result\"),\n",
    "    result_df.name == grouped_df_lower.name,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "\n",
    "# Add a unique ID to each DataFrame to facilitate the outer join\n",
    "result_df = result_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "non_matched_flight = non_matched_flight.withColumn(\"id\", monotonically_increasing_id())\n",
    "non_matched_grouped = non_matched_grouped.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "# Perform the outer joins using the unique IDs, result_df is now composed of matched airports\n",
    "enhanced_result_df = result_df.join(non_matched_flight, \"id\", \"outer\" ).join(non_matched_grouped, \"id\", \"outer\" )\n",
    "enhanced_result_df = enhanced_result_df.drop(\"id\")\n",
    "\n",
    "# Show the enhanced DataFrame with additional columns\n",
    "#enhanced_result_df.printSchema()\n",
    "\n",
    "# Select columns, get rid of duplicates\n",
    "selected_columns = [col for col in enhanced_result_df.columns if col != 'name' and col != 'departing_airport'] + ['grouped.name'] + ['flight.departing_airport']\n",
    "\n",
    "#will contain unmatched airports for each dataset\n",
    "enhanced_result_df = enhanced_result_df.select(selected_columns)\n",
    "enhanced_result_df.drop('name','departing_airport')\n",
    "#enhanced_result_df.show(n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7419b18-8499-4eb5-b65b-3b70dda46365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dataframe that contains airports matched and unmatched result from the join\n",
    "# Rename columns in result_df\n",
    "result_df = result_df.withColumnRenamed(\"name\", \"weather_matched\") \\\n",
    "                     .withColumnRenamed(\"departing_airport\", \"delay_matched\")\n",
    "\n",
    "# Rename columns in enhanced_result_df\n",
    "enhanced_result_df = enhanced_result_df.withColumnRenamed(\"name\", \"weather_unmatched\") \\\n",
    "                                       .withColumnRenamed(\"departing_airport\", \"delay_unmatched\")\n",
    "\n",
    "# Optional: If you need to ensure the rows are matched by order, add an index column to each DataFrame\n",
    "result_df = result_df.withColumn(\"index\", monotonically_increasing_id())\n",
    "enhanced_result_df = enhanced_result_df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Join DataFrames on the index column\n",
    "matched_and_unmatched_airports = result_df.join(\n",
    "    enhanced_result_df,\n",
    "    on=\"index\",\n",
    "    how=\"outer\"  # Use \"outer\" to include all rows from both DataFrames\n",
    ")\n",
    "\n",
    "# Drop the index column as it's no longer needed after joining\n",
    "matched_and_unmatched_airports = matched_and_unmatched_airports.drop(\"index\", 'name', 'departing_airport')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e793e46b-e808-4fea-ad76-6a90cef070c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count the amount of airports for each clumn in the enhanced dataset dataframe \n",
    "\n",
    "non_null_name_count = result_df.filter(col(\"name\").isNotNull()).count()\n",
    "non_null_name_count1 = result_df.filter(col(\"departing_airport\").isNotNull()).count()\n",
    "non_null_name_count2 = enhanced_result_df.filter(col(\"weather_unmatched\").isNotNull()).count()\n",
    "non_null_name_count3 = enhanced_result_df.filter(col(\"delay_unmatched\").isNotNull()).count()\n",
    "#print(\"Number of non-null strings in the 'name' column:\", non_null_name_count, non_null_name_count1,non_null_name_count2, non_null_name_count3)\n",
    "\n",
    "# Display the filtered DataFrame and print the counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf47076e-52b6-4634-8e03-35ced09409df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the parsed data\n",
    "data = []\n",
    "\n",
    "# Open the text file and parse it line by line\n",
    "with open('./airports.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line by comma to extract the needed parts\n",
    "        parts = line.split(',')\n",
    "        \n",
    "        # Check if the line has enough parts to avoid index errors\n",
    "        if len(parts) >= 4:\n",
    "            # Extract and clean the desired parts\n",
    "            # Remove quotation marks and extra spaces if present\n",
    "            name = parts[1].strip('\"').strip()\n",
    "            city = parts[2].strip('\"').strip()\n",
    "            country = parts[3].strip('\"').strip()\n",
    "            \n",
    "            # Combine the first two parts into one column, and keep the country as the second column\n",
    "            combined = f\"{name}, {city}, {country}\"\n",
    "            data.append(combined)\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df_airports = pd.DataFrame(data, columns=['Airport and City'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f08a73d9-8f5b-4070-9e51-0d8af85f4a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: fuzzywuzzy in /home/mate/.local/lib/python3.10/site-packages (0.18.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: python-Levenshtein in /home/mate/.local/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: Levenshtein==0.25.1 in /home/mate/.local/lib/python3.10/site-packages (from python-Levenshtein) (0.25.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in /home/mate/.local/lib/python3.10/site-packages (from Levenshtein==0.25.1->python-Levenshtein) (3.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c3c61f3-a54e-4edc-9d65-d4d2e84bf106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Get all airport names into one table\n",
    "df_delay_unique_airport = df_flightdelay.select('DEPARTING_AIRPORT').distinct().withColumnRenamed(\"DEPARTING_AIRPORT\", \"airport\")\n",
    "df_weather_unique_airport = df_weather.select('NAME').distinct().withColumnRenamed(\"NAME\", \"airport\")\n",
    "\n",
    "df_union = df_delay_unique_airport.union(df_weather_unique_airport)\n",
    "\n",
    "def filter_out_useless_parts_of_string(airport_name):\n",
    "    useless_words = [\"international\", \"airport\", \"regional\"]\n",
    "\n",
    "    modified = airport_name.lower()\n",
    "    for word in useless_words:\n",
    "        modified = modified.replace(word,\"\")\n",
    "        \n",
    "    return modified\n",
    "    \n",
    "filter_string_udf = udf(filter_out_useless_parts_of_string, StringType())\n",
    "\n",
    "df_union = df_union.select(filter_string_udf(col('airport'))).withColumnRenamed(\"filter_out_useless_parts_of_string(airport)\", \"airport\")\n",
    "#df_union.show(n=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8392322f-bd5d-47ea-ad10-763847e59700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "def get_matches(df1, col1, df2, col2, threshold=40):\n",
    "    # Convert each column to a list for processing, ensuring to drop NA values\n",
    "    list1 = df1[col1].dropna().tolist()\n",
    "    list2 = df2[col2].dropna().tolist()\n",
    "\n",
    "    # Find best matches with a score above the threshold\n",
    "    matches = []\n",
    "    for item in list1:\n",
    "        # Use process.extractOne to find the best match for each item from list1 in list2\n",
    "        best_match = process.extractOne(item, list2, scorer=fuzz.token_set_ratio)\n",
    "        if best_match and best_match[1] >= threshold:\n",
    "            matches.append((item, best_match[0], best_match[1]))\n",
    "\n",
    "    # Return matches as a DataFrame for better visualization\n",
    "    return pd.DataFrame(matches, columns=[col1, col2 + '_match', 'Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "935bd86e-6047-4e87-8d53-330b4cbd3419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add official airports using fuzzywuzzy\n",
    "# Assuming 'matched_and_unmatched_airports' is your PySpark DataFrame\n",
    "pandas_df = df_union.toPandas()  # Convert to Pandas DataFrame\n",
    "\n",
    "# Example usage (ensure df1 and df2 are already defined and loaded with your data)\n",
    "df_matches_airports = get_matches(pandas_df, 'airport', df_airports, 'Airport and City')\n",
    "\n",
    "spark_df_airports = spark.createDataFrame(df_matches_airports)\n",
    "# Show the DataFrame to verify conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5415602-ebd2-4bdd-a27b-13ec68b697a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joining_table = spark_df_airports.select(\"airport\", \"Airport and City_match\")\n",
    "#joining_table.show(n=2)\n",
    "delay_table = df_flightdelay.withColumn(\"DEPARTING_AIRPORT\", filter_string_udf('DEPARTING_AIRPORT'))\n",
    "#delay_table.show(n=2)\n",
    "weather_table = df_day_column.withColumn(\"NAME\", filter_string_udf('NAME'))\n",
    "#weather_table.show(n=2)\n",
    "\n",
    "#previous_arprt_table = df_flightdelay.withColumn(\"PREVIOUS_AIRPORT\", filter_string_udf('PREVIOUS_AIRPORT'))\n",
    "#previous_arprt_joined = previous_arprt_table.join(joining_table, joining_table.airport == previous_arprt_table.PREVIOUS_AIRPORT, 'inner')\n",
    "\n",
    "delay_joined = delay_table.join(joining_table, joining_table.airport == delay_table.DEPARTING_AIRPORT, 'inner')\n",
    "weather_joined = weather_table.join(joining_table, joining_table.airport == weather_table.NAME, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fe908b1-39e2-44d4-8c86-5408294eb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#delay_joined.show(n=3)\n",
    "#weather_joined.show(n=3)\n",
    "\n",
    "# Convert integer columns in df1 to strings\n",
    "weather_joined = weather_joined.withColumn(\"MONTH\", col(\"MONTH\").cast(\"string\")) \\\n",
    "         .withColumn(\"DAY_OF_WEEK\", col(\"DAY_OF_WEEK\").cast(\"string\"))\n",
    "\n",
    "result_joined = delay_joined.join(weather_joined, [\"MONTH\",\"DAY_OF_WEEK\",\"Airport and City_match\"], 'inner')\n",
    "\n",
    "#result_joined.show(n=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2c69994-b87c-4754-9200-b20cbcb50dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "# LATITUDE, LONGITUDE, STATION, MONTH, airport, normalized_name, NAME, _c0\n",
    "result_joined = result_joined.drop(\"LATITUDE\", \"LONGITUDE\", \"STATION\", \"MONTH\", \\\n",
    "                                   \"airport\", \"normalized_name\", \"NAME\", \"_c0\", \\\n",
    "                                   \"DATE\", \"AIRLINE_FLIGHTS_MONTH\", \"AVG_MONTHLY_PASS_AIRLINE\" \\\n",
    "                                   \"DEPARTING_AIRPORT\", \"PGTM\", \"WDF5\", \"WDF2\", \"WSF2\", \"WSF5\", \\\n",
    "                                   \"SN32\", \"SX32\", \"TOBS\",\"WESD\", \"PSUN\",\"TSUN\")\n",
    "\n",
    "df = result_joined\n",
    "#result_joined.count()\n",
    "#result_joined.select(\"CONCURRENT_FLIGHTS\").filter(col(\"CONCURRENT_FLIGHTS\").isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bc58762-a744-44fe-9c6e-8272a8e5e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"TMIN\", df[\"TMIN\"].cast(\"float\"))\n",
    "df = df.withColumn(\"PRCP\", df[\"PRCP\"].cast(\"float\"))\n",
    "\n",
    "\n",
    "#print(df.count())\n",
    "\n",
    "\n",
    "#count = df.filter(((col(\"PRCP\") > 0.0) & (col(\"TMIN\") < 3.0)) & (col(\"SNOW\").isNull() | (col(\"SNOW\") == ''))).count()\n",
    "\n",
    "df = df.withColumn(\"SNOW\", when(col(\"TMIN\") > 3, 0).otherwise(col(\"SNOW\")))\n",
    "df = df.withColumn(\"SNOW\", when( \\\n",
    "                    (col(\"SNOW\").isNull() | (col(\"SNOW\") == '')) & (col(\"PRCP\") > 0), col(\"PRCP\") \\\n",
    "                               ).otherwise(lit(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cfb6937-44ac-49c8-b956-12837d92fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify temperatures, create new flag column EXTREME_WEATHER based on TMIN and TMAX and drop all others\n",
    "\n",
    "\n",
    "# Since TMAX and TMIN are strings, you need to convert them to integers before comparison\n",
    "df = df.withColumn(\"TMAX\", df[\"TMAX\"].cast(\"integer\"))\n",
    "df = df.withColumn(\"TMIN\", df[\"TMIN\"].cast(\"integer\"))\n",
    "\n",
    "# Creating the EXTREME_WEATHER column based on the conditions provided\n",
    "df = df.withColumn(\"EXTREME_WEATHER\", \n",
    "                   when((col(\"TMAX\") > 40) | (col(\"TMIN\") < 0), 1)\n",
    "                   .otherwise(0))\n",
    "\n",
    "df = df.drop(\"TMIN\", \"TMAX\", \"TAVG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2395a6b-e573-4f21-bcc3-61898a3f3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Replace all WT** with column which adds these extreme weather conditions into one value\n",
    "values_as_strings = [f\"WT{i:02}\" for i in range(1, 12)]\n",
    "#print(values_as_strings)\n",
    "#print(\"+\".join(values_as_strings))\n",
    "\n",
    "for column_name in values_as_strings:\n",
    "    df = df.withColumn(column_name, df[column_name].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c36c5ab-63ce-4591-96e2-7780413559a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02f966b1-c944-4e85-bdc6-cfda4eede96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "total_wt_column = reduce(lambda a, b: a + b, [coalesce(col(c), lit(0)) for c in values_as_strings])\n",
    "\n",
    "df = df.withColumn('EXTREME_WEATHER_WT', total_wt_column)\n",
    "\n",
    "df = df.drop('WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT07', 'WT08', 'WT09', 'WT10', 'WT11')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16071707-3de0-41a5-80bb-f55cb3982655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- Airport and City_match: string (nullable = true)\n",
      " |-- DEP_DEL15: string (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- DISTANCE_GROUP: string (nullable = true)\n",
      " |-- SEGMENT_NUMBER: string (nullable = true)\n",
      " |-- CONCURRENT_FLIGHTS: string (nullable = true)\n",
      " |-- NUMBER_OF_SEATS: string (nullable = true)\n",
      " |-- CARRIER_NAME: string (nullable = true)\n",
      " |-- AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AIRLINE_AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRPORT: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRLINE: string (nullable = true)\n",
      " |-- FLT_ATTENDANTS_PER_PASS: string (nullable = true)\n",
      " |-- GROUND_SERV_PER_PASS: string (nullable = true)\n",
      " |-- PLANE_AGE: string (nullable = true)\n",
      " |-- DEPARTING_AIRPORT: string (nullable = true)\n",
      " |-- PREVIOUS_AIRPORT: string (nullable = true)\n",
      " |-- AWND: string (nullable = true)\n",
      " |-- PRCP: float (nullable = true)\n",
      " |-- SNOW: float (nullable = true)\n",
      " |-- SNWD: string (nullable = true)\n",
      " |-- DATE_NEW: date (nullable = true)\n",
      " |-- EXTREME_WEATHER: integer (nullable = false)\n",
      " |-- EXTREME_WEATHER_WT: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7fdc75a-e3cc-481f-967d-469dff1dc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# Define a function to count nulls and empty strings\n",
    "def count_nulls_and_empties(df):\n",
    "    # Use aggregation to sum up each condition of being null or empty across all columns\n",
    "    exprs = [count(when(df[c].isNull() | (df[c] == \"\"), c)).alias(c) for c in df.columns]\n",
    "    return df.agg(*exprs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a1a8912-5e86-4fcc-8fda-2702c487b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values, for example AWND and PRCP\n",
    "\n",
    "from pyspark.sql.functions import avg, col, coalesce, month, median\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window spec partitioned by month\n",
    "window_spec = Window.partitionBy(month(\"DATE_NEW\"))\n",
    "\n",
    "# Assuming 'column_name' is the column with null values you want to fill\n",
    "avg_column = avg(col(\"AWND\")).over(window_spec)\n",
    "avg_prcp = avg(col(\"PRCP\")).over(window_spec)\n",
    "med_column = median(col(\"FLT_ATTENDANTS_PER_PASS\")).over(window_spec)\n",
    "\n",
    "\n",
    "# Replace nulls with the average of that month\n",
    "df = df.withColumn(\"AWND_filled\", coalesce(col(\"AWND\"), avg_column))\n",
    "df = df.withColumn(\"PRCP_filled\", coalesce(col(\"PRCP\"), avg_prcp))\n",
    "df = df.withColumn(\"DISTANCE_GROUP_filled\", coalesce(col(\"DISTANCE_GROUP\"), lit(\"1\")))\n",
    "df = df.withColumn(\"FLT_ATTENDANTS_PER_PASS_filled\", coalesce(col(\"FLT_ATTENDANTS_PER_PASS\"), med_column))\n",
    "\n",
    "\n",
    "df = df.drop(\"AWND\").withColumnRenamed(\"AWND_filled\", \"AWND\")\n",
    "df = df.drop(\"PRCP\").withColumnRenamed(\"PRCP_filled\", \"PRCP\")\n",
    "df = df.drop(\"DISTANCE_GROUP\").withColumnRenamed(\"DISTANCE_GROUP_filled\", \"DISTANCE_GROUP\")\n",
    "df = df.drop(\"FLT_ATTENDANTS_PER_PASS\").withColumnRenamed(\"FLT_ATTENDANTS_PER_PASS_filled\", \"FLT_ATTENDANTS_PER_PASS\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64a5bfb2-6267-43d9-9a98-346b022ed8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Replace null values in the 'PREVIOUS_AIRPORT' column with empty strings\n",
    "df = df.withColumn('PREVIOUS_AIRPORT', when(col('PREVIOUS_AIRPORT').isNull(), '').otherwise(col('PREVIOUS_AIRPORT')))\n",
    "#df.show(n=2)\n",
    "#nulls_and_empties_count = count_nulls_and_empties(df)\n",
    "\n",
    "#nulls_and_empties_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a075780-d721-40c3-9517-ccffafbd1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "calculate_flights = df.select(\"CARRIER_NAME\", \"Airport and City_match\", \"DATE_NEW\")\n",
    "\n",
    "calculate_flights = calculate_flights.withColumn(\"MONTH\", month(col(\"DATE_NEW\").alias(\"MONTH\")))\n",
    "\n",
    "count = calculate_flights.groupBy(\"CARRIER_NAME\", \"Airport and City_match\", \"MONTH\").count()\n",
    "#print(count.show())\n",
    "\n",
    "count = count.groupBy(\"CARRIER_NAME\", \"Airport and City_match\") \\\n",
    "                   .agg(round(avg(\"count\")).alias(\"monthly_avg_count\"))\n",
    "#print(count.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "237a0403-0565-4b6b-b853-bd1a77a38289",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsdf = df.join(\n",
    "    count,\n",
    "    [\"CARRIER_NAME\", \"Airport and City_match\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "rsdf = rsdf.withColumn(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\",\\\n",
    "                       when(\\\n",
    "                           (col(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\").isNull() | (col(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\") == ''))\n",
    "                           , col(\"monthly_avg_count\")).otherwise(col(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\")))\n",
    "\n",
    "rsdf = rsdf.drop(\"monthly_avg_count\")\n",
    "\n",
    "#rsdf.show()\n",
    "\n",
    "result_df = result_joined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf93c537-7fbd-4dc7-85cf-9e0c887ec911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rsdf\n",
    "\n",
    "#nulls_and_empties_count = count_nulls_and_empties(df)\n",
    "\n",
    "#nulls_and_empties_count.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a5c1ed2-4caa-469a-817a-d5f255941d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use aggregation to sum up each condition of being null or empty across all columns\n",
    "#exprs = [count(when(df[c].isNull() | (df[c] == \"\"), c)).alias(c) for c in df.columns]\n",
    "#df.agg(*exprs).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01aa0cc6-5c29-41b8-91a1-da199cf4b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Converting from numerical to nominal\n",
    "# ------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cae857cd-5074-426f-9b12-da9a6f0e4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert precipitation from numerical to nominal\n",
    "\n",
    "# Calculate the quantile thresholds\n",
    "#thresholds = result_df.approxQuantile(\"PRCP\", [0.33, 0.67], 0.01)  # 0.01 is the relative error\n",
    "\n",
    "# Categorize based on quantile thresholds\n",
    "#result_df = result_df.withColumn(\n",
    "#    \"precip_category\",\n",
    "#    when(col(\"PRCP\") <= thresholds[0], \"low\")\n",
    "#    .when(col(\"PRCP\") <= thresholds[1], \"medium\")\n",
    "#    .otherwise(\"high\")\n",
    "#)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "#result_df.select(\"PRCP\", \"precip_category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0676654e-66b7-41ae-a093-f10e7832b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform weekday from numerical to nominal\n",
    "\n",
    "# Weekday mapping dictionary\n",
    "month_dict = {\n",
    "    '1': 'Monday', '2': 'Tuesday', '3': 'Wednesday', '4': 'Thursday', \n",
    "    '5': 'Friday', '6': 'Saturday', '7': 'Sunday'}\n",
    "\n",
    "# Define the UDF to convert numerical months to names\n",
    "def convert_weekday_to_name(weekday):\n",
    "    return month_dict.get(str(weekday), \"Unknown\")\n",
    "\n",
    "convert_weekday_udf = udf(convert_weekday_to_name, StringType())\n",
    "\n",
    "# Apply the UDF to create a new column with month names\n",
    "df_with_months = result_df.withColumn(\"DAY_OF_WEEK_NAME\", convert_weekday_udf(result_df[\"DAY_OF_WEEK\"]))\n",
    "#df_with_months.show(n=1)\n",
    "df_with_months = df_with_months.drop(\"DAY_OF_WEEK\").withColumnRenamed(\"DAY_OF_WEEK_NAME\", \"DAY_OF_WEEK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85d45649-18f7-41b7-bd6e-2ecee5b3df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER_OF_SEATS into nominal\n",
    "\n",
    "# Categorize based on research\n",
    "result_df = result_df.withColumn(\n",
    "    \"NUMBER_OF_SEATS_NOM\",\n",
    "    when(col(\"NUMBER_OF_SEATS\") <= 100, \"Small\")\n",
    "    .when(col(\"NUMBER_OF_SEATS\") <= 200, \"Medium\")\n",
    "    .when(col(\"NUMBER_OF_SEATS\") <= 400, \"Large\")\n",
    "    .otherwise(\"Jumbo\")\n",
    ")\n",
    "\n",
    "# Replace NUMBER_OF_SEATS column with the nominal one\n",
    "result_df = result_df.drop(\"NUMBER_OF_SEATS\").withColumnRenamed(\"NUMBER_OF_SEATS_NOM\", \"NUMBER_OF_SEATS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d43c8aee-bbab-4d38-a8d9-37cb1293e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plane age into nominal\n",
    "\n",
    "# Categorize based on research\n",
    "result_df = result_df.withColumn(\n",
    "    \"PLANE_AGE_NOM\",\n",
    "    when(col(\"PLANE_AGE\") <= 10, \"New\")\n",
    "    .when(col(\"PLANE_AGE\") <= 20, \"Standard\")\n",
    "    .otherwise(\"Old\")\n",
    ")\n",
    "\n",
    "# Replace PLANE_AGE column with the nominal one\n",
    "result_df = result_df.drop(\"PLANE_AGE\").withColumnRenamed(\"PLANE_AGE_NOM\", \"PLANE_AGE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2c735e4-60af-4f66-8712-aa54de234f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CARRIER_NAME: string (nullable = true)\n",
      " |-- Airport and City_match: string (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- DEP_DEL15: string (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- SEGMENT_NUMBER: string (nullable = true)\n",
      " |-- CONCURRENT_FLIGHTS: string (nullable = true)\n",
      " |-- NUMBER_OF_SEATS: string (nullable = true)\n",
      " |-- AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AIRLINE_AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRPORT: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRLINE: string (nullable = true)\n",
      " |-- GROUND_SERV_PER_PASS: string (nullable = true)\n",
      " |-- PLANE_AGE: string (nullable = true)\n",
      " |-- DEPARTING_AIRPORT: string (nullable = true)\n",
      " |-- PREVIOUS_AIRPORT: string (nullable = true)\n",
      " |-- SNOW: float (nullable = true)\n",
      " |-- SNWD: string (nullable = true)\n",
      " |-- DATE_NEW: date (nullable = true)\n",
      " |-- EXTREME_WEATHER: integer (nullable = false)\n",
      " |-- EXTREME_WEATHER_WT: integer (nullable = false)\n",
      " |-- AWND: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- DISTANCE_GROUP: string (nullable = false)\n",
      " |-- FLT_ATTENDANTS_PER_PASS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df  = df\n",
    "result_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b8953a3-a05f-40c2-bbab-7bba3b16ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Data analysis\n",
    "# ------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e96b48-06b3-49be-9bb1-44039be849b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate staistics for numerical attributes\n",
    "numeric_columns = [\n",
    "     \"DEP_DEL15\", \"SEGMENT_NUMBER\", \"CONCURRENT_FLIGHTS\",\n",
    "    \"AIRPORT_FLIGHTS_MONTH\",\n",
    "    \"AIRLINE_AIRPORT_FLIGHTS_MONTH\", \"AVG_MONTHLY_PASS_AIRPORT\",\n",
    "    \"GROUND_SERV_PER_PASS\", \"SNOW\", \"EXTREME_WEATHER\", \"EXTREME_WEATHER_WT\",\n",
    "    \"AWND\", \"PRCP\", \"DISTANCE_GROUP\", \"FLT_ATTENDANTS_PER_PASS\",\"SNWD\"\n",
    "]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cast numeric columns to integer\n",
    "for column in numeric_columns:\n",
    "    df_stat = df.withColumn(column, col(column).cast('float'))\n",
    "#df_stat.show(n=2)\n",
    "#stats_df = df.describe()\n",
    "#stats_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5669733-0662-4245-8d6c-92329b6d6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 10% sample\n",
    "\n",
    "fractions = df.groupBy(\"DEP_DEL15\").count().withColumnRenamed(\"count\", \"fraction\").withColumn(\"fraction\", col(\"fraction\") / df.count())\n",
    "\n",
    "# Sample the DataFrame using the calculated fractions\n",
    "sampled_df = df.stat.sampleBy(\"DEP_DEL15\", fractions.select(\"DEP_DEL15\", \"fraction\").rdd.collectAsMap(), seed=1234)\n",
    "\n",
    "# Show the sampled data distribution\n",
    "#sampled_df.groupBy(\"DEP_DEL15\").count().show()\n",
    "#df.count()\n",
    "#sampled_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712cadd-37d4-41bb-bfc3-cb0cd7aa280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10% of dataset\n",
    "original_size = df\n",
    "df = sampled_df\n",
    "#df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81c809-26e4-47ae-aa85-e2fb60f4d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # For better visual aesthetics\n",
    "\n",
    "# Setting a style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# List of nominal columns\n",
    "nominal_columns = [\n",
    "    \"CARRIER_NAME\",\n",
    "    \"Airport and City_match\",\n",
    "    \"DEP_TIME_BLK\",\n",
    "    \"DEPARTING_AIRPORT\",\n",
    "    \"PREVIOUS_AIRPORT\",\n",
    "    \"DATE_NEW\",\n",
    "    \"PLANE_AGE\"\n",
    "]\n",
    "\n",
    "# Create a figure to hold the subplots\n",
    "fig, axes = plt.subplots(nrows=len(nominal_columns), figsize=(12, 6 * len(nominal_columns)))  # Adjusted size for better fit\n",
    "\n",
    "# Loop through each nominal column and create a histogram\n",
    "for i, column in enumerate(nominal_columns):\n",
    "    # Count the occurrences of each category in the column\n",
    "    category_counts = df.groupBy(column).count().toPandas().sort_values(by='count', ascending=False)\n",
    "\n",
    "    # If there are many categories, you might want to limit the number of categories displayed\n",
    "    if len(category_counts) > 20:  # Adjust the threshold as needed\n",
    "        category_counts = category_counts.head(20)  # Only show top 20 categories\n",
    "    \n",
    "    # Plotting the histogram for the column\n",
    "    sns.barplot(x=column, y='count', data=category_counts, ax=axes[i], palette='viridis')  # Using seaborn for a better-looking plot\n",
    "    axes[i].set_title(f'Histogram of {column}')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Counts')\n",
    "    axes[i].tick_params(axis='x', rotation=45)  # Rotate labels to prevent overlap\n",
    "\n",
    "# Adjust layout to prevent overlap of subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#\n",
    "#CARRIER_NAME: 7.594988367110728e-05\n",
    "\n",
    "#Information Gain for Airport and City_match: 8.950570449908421e-05\n",
    "\n",
    "#Information Gain for DEP_TIME_BLK: 0.00044215567622180735\n",
    "\n",
    "#Information Gain for DEPARTING_AIRPORT: 8.933648488036738e-05\n",
    "\n",
    "#Information Gain for PREVIOUS_AIRPORT: 0.00036900671974516436\n",
    "Information Gain for NUMBER_OF_SEATS: 0.00012451283453875778\n",
    "\n",
    "                                                                                \n",
    "\n",
    "Information Gain for PLANE_AGE: -0.00014759181821908524\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f80505-7af2-462a-be76-ecc183bec053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from math import log2\n",
    "\n",
    "def entropy(probability):\n",
    "    \"\"\"Calculate the entropy based on probability.\"\"\"\n",
    "    return -probability * log2(probability)\n",
    "\n",
    "def calculate_entropy(df, column):\n",
    "    \"\"\"Calculate the entropy of a given DataFrame column.\"\"\"\n",
    "    total_count = df.count()\n",
    "    probabilities = df.groupBy(column).agg((count('*') / total_count).alias('probability'))\n",
    "    entropy_udf = udf(entropy, DoubleType())\n",
    "    entropy_df = probabilities.withColumn('entropy_part', entropy_udf('probability'))\n",
    "    total_entropy = entropy_df.agg(sqlsum('entropy_part')).collect()[0][0]\n",
    "    return total_entropy if total_entropy is not None else 0\n",
    "\n",
    "def information_gain(df, attribute, target):\n",
    "    \"\"\"Calculate the information gain of a column with respect to the target.\"\"\"\n",
    "    total_entropy = calculate_entropy(df, target)\n",
    "    total_count = df.count()\n",
    "\n",
    "    attribute_values = df.select(attribute).distinct().collect()\n",
    "\n",
    "    conditional_entropy_sum = 0\n",
    "    for value_row in attribute_values:\n",
    "        value = value_row[0]\n",
    "        subset_df = df.filter(col(attribute) == value)\n",
    "        subset_count = subset_df.count()\n",
    "        \n",
    "        # Ensure subset has sufficient data to compute entropy\n",
    "        if subset_count > 0:\n",
    "            subset_entropy = calculate_entropy(subset_df, target)\n",
    "            conditional_entropy_sum += (subset_count / total_count) * subset_entropy\n",
    "\n",
    "    return total_entropy - conditional_entropy_sum\n",
    "\n",
    "\n",
    "# Example usage\n",
    "nominal_columns = [\"NUMBER_OF_SEATS\", \"PLANE_AGE\"]\n",
    "\n",
    "information_gains = {}\n",
    "for column in nominal_columns:\n",
    "    ig = information_gain(df, column, 'DEP_DEL15')  # Assuming DEP_DEL15 is your target column\n",
    "    information_gains[column] = ig\n",
    "    print(f'Information Gain for {column}: {ig}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723b598-5ad4-44ba-ab28-a692d104c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe33646-1581-4c85-8187-63aab2482d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Model creation and training\n",
    "# ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089d2b5-1945-4bc7-9110-41e8bba691a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"DEPARTING_AIRPORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49816900-c826-435a-a3c3-c652208b9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plane age into numerical\n",
    "\n",
    "# Categorize based on research\n",
    "df = df.withColumn(\n",
    "    \"PLANE_AGE_NOM\",\n",
    "    when(col(\"PLANE_AGE\") == \"New\", 1)\n",
    "    .when(col(\"PLANE_AGE\") == \"Standard\", 2)\n",
    "    .otherwise(3)\n",
    ")\n",
    "\n",
    "# Replace PLANE_AGE column with the nominal one\n",
    "df = df.drop(\"PLANE_AGE\").withColumnRenamed(\"PLANE_AGE_NOM\", \"PLANE_AGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15198f-02e3-423a-95a3-1b9fbfc5d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER_OF_SEATS into numerical category\n",
    "\n",
    "# Categorize based on research\n",
    "result_df = df.withColumn(\n",
    "    \"NUMBER_OF_SEATS_NOM\",\n",
    "    when(col(\"NUMBER_OF_SEATS\") == \"Small\", 1)\n",
    "    .when(col(\"NUMBER_OF_SEATS\") == \"Medium\", 2)\n",
    "    .when(col(\"NUMBER_OF_SEATS\") == \"Large\", 3)\n",
    "    .otherwise(4)\n",
    ")\n",
    "\n",
    "# Replace NUMBER_OF_SEATS column with the nominal one\n",
    "result_df = result_df.drop(\"NUMBER_OF_SEATS\").withColumnRenamed(\"NUMBER_OF_SEATS_NOM\", \"NUMBER_OF_SEATS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99569c96-3230-4d7a-81de-0aa41781cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979958c-96f1-4feb-8bcc-c5ac5bc30b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAY_OF_WEEK into numerical form\n",
    "from pyspark.sql.functions import col, radians, cos, sin\n",
    "\n",
    "df = df.withColumn(\"day_cos\", cos(radians(col(\"DAY_OF_WEEK\") * (360/7))))\n",
    "df = df.withColumn(\"day_sin\", sin(radians(col(\"DAY_OF_WEEK\") * (360/7))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1422333-d797-4dc1-b1db-b2e0f29ba057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FOUND THE CULPRIT OF NULLS, DROPPING SNWD \n",
    "df.drop(\"SNWD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1abdd3-3841-4a5d-a2a4-53840b804fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cast to float what can be casted\n",
    "float_columns = [\"DEP_DEL15\", \"SEGMENT_NUMBER\", \"CONCURRENT_FLIGHTS\", \"AIRPORT_FLIGHTS_MONTH\",\n",
    "                 \"AIRLINE_AIRPORT_FLIGHTS_MONTH\", \"AVG_MONTHLY_PASS_AIRPORT\", \"GROUND_SERV_PER_PASS\",\n",
    "                 \"SNOW\", \"EXTREME_WEATHER_WT\", \"AWND\", \"PRCP\", \"DISTANCE_GROUP\", \"FLT_ATTENDANTS_PER_PASS\", \"day_cos\",\"day_sin\",\"NUMBER_OF_SEATS\",\"AVG_MONTHLY_PASS_AIRLINE\"]\n",
    "\n",
    "for col_name in float_columns:\n",
    "    df = df.withColumn(col_name, df[col_name].cast('float'))\n",
    "\n",
    "# Check the schema of the new DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf5f4a-3a9c-4fe7-aa09-7e318b39557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nulls_and_empties_count = count_nulls_and_empties(df.select(\"AVG_MONTHLY_PASS_AIRPORT\", \"GROUND_SERV_PER_PASS\",\"SNOW\", \"SNWD\"))\n",
    "\n",
    "#nulls_and_empties_count.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34276217-f159-4dd6-89a2-aca3589ec084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Indexing and Encoding for 'CARRIER_NAME'\n",
    "carrier_name_indexer = StringIndexer(inputCol=\"CARRIER_NAME\", outputCol=\"CARRIER_NAME_Index\",handleInvalid=\"skip\")\n",
    "carrier_name_encoder = OneHotEncoder(inputCol=\"CARRIER_NAME_Index\", outputCol=\"CARRIER_NAME_Encoded\")\n",
    "\n",
    "# Indexing and Encoding for 'PREVIOUS_AIRPORT'\n",
    "#prev_airport_indexer = StringIndexer(inputCol=\"PREVIOUS_AIRPORT\", outputCol=\"PREVIOUS_AIRPORT_Index\")\n",
    "#prev_airport_encoder = OneHotEncoder(inputCol=\"PREVIOUS_AIRPORT_Index\", outputCol=\"PREVIOUS_AIRPORT_Encoded\")\n",
    "\n",
    "# Indexing and Encoding for 'Airport and City_match'\n",
    "city_match_indexer = StringIndexer(inputCol=\"Airport and City_match\", outputCol=\"Airport_and_City_match_Index\",handleInvalid=\"skip\")\n",
    "city_match_encoder = OneHotEncoder(inputCol=\"Airport_and_City_match_Index\", outputCol=\"Airport_and_City_match_Encoded\")\n",
    "\n",
    "#add later \"PREVIOUS_AIRPORT_Encoded\"\n",
    "#removed ground per pass,\"AIRPORT_FLIGHTS_MONTH\"\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[ \"Airport_and_City_match_Encoded\", \"CARRIER_NAME_Encoded\"] + [\"SNOW\",\"SEGMENT_NUMBER\", \"CONCURRENT_FLIGHTS\", \"NUMBER_OF_SEATS\",\n",
    "               \"AIRLINE_AIRPORT_FLIGHTS_MONTH\", \"AVG_MONTHLY_PASS_AIRPORT\",\n",
    "               \"AWND\", \"PRCP\", \"DISTANCE_GROUP\", \"FLT_ATTENDANTS_PER_PASS\", \"day_cos\",\"day_sin\", \"EXTREME_WEATHER_WT\"],  # Replace \"other_features\" with actual feature names\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Pipeline all preprocessing steps\n",
    "pipeline = Pipeline(stages=[\n",
    "      city_match_indexer, city_match_encoder, carrier_name_indexer, carrier_name_encoder, assembler\n",
    "])\n",
    "\n",
    "# Fit and Transform\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# Unclear if works correctly\n",
    "#transformed_df = model.transform(df)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b7dd6-2969-4242-a07c-b7f1ac27a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checkpoint = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e15029-e279-406b-9451-39fa03ca6482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_df = model.transform(df)\n",
    "#transformed_df.show()\n",
    "transformed_df.drop(\"PREVIOUS_AIRPORT\",\"CARRIER_NAME\",\"Airport and City_match\",\"DAY_OF_WEEK\")\n",
    "\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Convert DATE_NEW to DateType\n",
    "transformed_df = transformed_df.withColumn(\"DATE_NEW\", to_date(\"DATE_NEW\"))\n",
    "\n",
    "# Convert EXTREME_WEATHER to FloatType (assuming it's a binary indicator)\n",
    "transformed_df = transformed_df.withColumn(\"EXTREME_WEATHER\", col(\"EXTREME_WEATHER\").cast(\"float\"))\n",
    "#transformed_df.printSchema()\n",
    "k_means_df = transformed_df.select(\"features\")\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "\n",
    "def check_features_format(df):\n",
    "    \"\"\"\n",
    "    Check if the 'features' column in the DataFrame contains properly formatted SparseVectors.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame containing a 'features' column.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if all vectors in the 'features' column are properly formatted SparseVectors, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check if the 'features' column exists in the DataFrame\n",
    "    if 'features' not in df.columns:\n",
    "        print(\"Error: 'features' column not found in the DataFrame.\")\n",
    "        return False\n",
    "    \n",
    "    # Check if all values in the 'features' column are SparseVectors\n",
    "    all_sparse = df.select('features').rdd.map(lambda row: isinstance(row.features, SparseVector)).reduce(lambda x, y: x and y)\n",
    "    \n",
    "    return all_sparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as dpd\n",
    "\n",
    "\n",
    "\n",
    "df = k_means_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6dad6-9fb2-4f3e-8b7c-1132e95893f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1293295a-d532-4096-a775-4bd333e147be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.limit(10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beffd78e-acd4-4ca0-9b59-64a3592a34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType, FloatType, ArrayType\n",
    "from pyspark.ml.linalg import VectorUDT, DenseVector\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import sqrt, pow, col\n",
    "\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "\n",
    "kmeans = KMeans()\n",
    "model = kmeans.fit(df)\n",
    "model.summary.predictions.show()\n",
    "\n",
    "\n",
    "print(model.clusterCenters())\n",
    "\n",
    "\n",
    "print(model.summary.clusterSizes)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43989ca-ac48-442d-b32a-c40677fff3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into training (60%) and test (40%) sets\n",
    "train_df, test_df = df.randomSplit([0.6, 0.4], seed=1234)\n",
    "\n",
    "# Show the size of each set\n",
    "print(\"Training Dataset Count: \" + str(train_df.count()))\n",
    "print(\"Testing Dataset Count: \" + str(test_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e9988-2aa5-44a7-a79e-2615b5c63037",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample the DataFrame using the calculated fractions\n",
    "#smaller_sample, rest_of_df =  df.randomSplit([0.1, 0.9], seed=1234)\n",
    "\n",
    "# Split the DataFrame into training (60%) and test (40%) sets\n",
    "train_data, test_data = df.limit(1000000).randomSplit([0.6, 0.4], seed=1234)\n",
    "\n",
    "# Show the size of each set\n",
    "print(\"Training Dataset Count: \" + str(train_data.count()))\n",
    "print(\"Testing Dataset Count: \" + str(test_data.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca4b422-7f9e-450d-9436-cc4206df05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### AT THIS POINT DEFINE train_data AND test_data\n",
    "# Like so for example: train_data, test_data = final_data.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93845994-30e5-453c-a8d2-3c65ee06984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes model\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "# Indexing string columns to convert to numeric values for model input\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df)\n",
    "    for column in list(set(df.columns) - set(['DEP_DEL15'])) # Assuming 'DEP_DEL15' is the label\n",
    "]\n",
    "\n",
    "# Use Pipeline to apply indexers\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_r = pipeline.fit(df).transform(df)\n",
    "\n",
    "# VectorAssembler to create feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[column + \"_index\" for column in list(set(df.columns) - set(['DEP_DEL15']))],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(df_r)\n",
    "\n",
    "# Select features and label for model training\n",
    "final_data = data.select(col(\"features\"), col(\"DEP_DEL15\").alias(\"label\"))\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# Train the model\n",
    "model = nb.fit(train_data)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76577fe5-c96a-49db-bce4-38465beb7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Initialize the LinearSVC model\n",
    "\n",
    "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=100, regParam=0.1)\n",
    "\n",
    "# Fit the model on training data\n",
    "svm_model = svm.fit(train_data)\n",
    "\n",
    "# Predictions\n",
    "predictions = svm_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Area under ROC: {auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054460c-2b7e-410c-b3d2-0136b05f9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67654b31-00ad-4679-b1d3-9bd74353c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 135:=====>          (4 + 1) / 12][Stage 139:>                (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "#train random forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "# Assuming your dataframe is called 'df' and your feature columns are in 'X' and label column in 'y'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['feature1', 'feature2', ...]], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "# Initialize and train the Random Forest classifier with parallel processing\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Compute precision, recall, F1 score, and MCC\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Print or use these metrics as needed\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Matthews Correlation Coefficient:\", mcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544459ec-e06f-4dda-a925-8068eaec1665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
