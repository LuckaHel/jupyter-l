{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "118939be-42d4-4a27-b4df-758358c8e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_date, concat, lit\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/mate/.local/lib/python3.10/site-packages/pyspark/\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628aa5bc-3116-432b-836b-b7180bf82f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 21:28:35 WARN Utils: Your hostname, ces-shrd-1 resolves to a loopback address: 127.0.1.1; using 192.168.1.25 instead (on interface wlp0s20f3)\n",
      "24/05/04 21:28:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/04 21:28:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"My Spark Application\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01556bb0-76d9-41a3-8586-e6275b034fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, dayofweek,to_date, month, count, avg\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number,   sum, when\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path_flightdelay = \"./full_data_flightdelay.csv\"  # Replace with the path to your CSV file\n",
    "\n",
    "\n",
    "df_flightdelay = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(csv_file_path_flightdelay)\n",
    "\n",
    "\n",
    "# Read the CSV file using the manually defined schema\n",
    "csv_file_path_weather = \"./airport_weather_2019.csv\"  # Replace with your file path\n",
    "df_weather = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(csv_file_path_weather)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab71331e-dd5a-4fe4-96d8-971c711c5836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather.select('DATE').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34ea6c03-0380-4397-a6b4-c138cc21647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Data cleanup and preparation\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c09dbf2-39ce-42c3-9e35-35ffc9c47fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 21:28:40 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# create new column for month and day_of_week values derived from date\n",
    "df_day_column = df_weather.withColumn(\"DATE_NEW\", to_date(col(\"DATE\"), \"M/d/yyyy\"))\n",
    "df_day_column = df_day_column.withColumn(\"DATE_NEW\", coalesce(df_day_column[\"DATE_NEW\"], to_date(df_day_column[\"DATE\"], 'yyyy-MM-dd')))\n",
    "\n",
    "df_day_column = df_day_column.withColumn(\"DAY_OF_WEEK\", dayofweek(col(\"DATE_NEW\").alias(\"DAY_OF_WEEK\")))\n",
    "df_day_column = df_day_column.withColumn(\"MONTH\", month(col(\"DATE_NEW\").alias(\"MONTH\")))\n",
    "#df_day_column = df_weather.withColumn(\"DAY_OF_WEEK\", dayofweek(col(\"DATE\").alias(\"DAY_OF_WEEK\"))) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df_day_column.show(n=2)\n",
    "df_day_column.createOrReplaceTempView(\"table1\")\n",
    "df_select = spark.sql(\"SELECT STATION, NAME,DAY_OF_WEEK,DATE, MONTH, AWND, PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, WDF2 from table1\")\n",
    "#df_select.show(n=5)\n",
    "\n",
    "grouped_df = df_select.groupBy(\"MONTH\", \"NAME\").agg(\n",
    "    avg(\"AWND\").alias(\"AWND\"),\n",
    "    avg(\"PRCP\").alias(\"PRCP\"),\n",
    "    avg(\"SNOW\").alias(\"SNOW\"),\n",
    "    avg(\"SNWD\").alias(\"SNWD\"),\n",
    "    avg(\"TAVG\").alias(\"TAVG\"),\n",
    "    avg(\"TMAX\").alias(\"TMAX\"),\n",
    "    avg(\"TMIN\").alias(\"TMIN\"),\n",
    "    avg(\"WDF2\").alias(\"WDF2\")\n",
    ").orderBy(\"NAME\",\"MONTH\")\n",
    "\n",
    "\n",
    "#grouped_df.show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "426ff018-c155-4794-b558-80e9545644ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_day_column.select('DATE_NEW').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "983d5e23-d23d-49cc-b55b-1427c1e118ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, split, col, lit, monotonically_increasing_id\n",
    "\n",
    "\n",
    "# Normalize joining columns\n",
    "grouped_df = grouped_df.withColumn(\"normalized_name\", lower(col(\"name\")))\n",
    "df_flightdelay = df_flightdelay.withColumn(\"normalized_name\", lower(split(col(\"departing_airport\"), \" \").getItem(0)))\n",
    "\n",
    "# Group by to investigate\n",
    "grouped_df_nn = grouped_df.groupBy(\"normalized_name\").agg(\n",
    "    count('*').alias('count')\n",
    ")\n",
    "\n",
    "grouped_df_name = grouped_df.groupBy(\"NAME\").agg(\n",
    "    count('*').alias('count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66837d93-05eb-4042-bf6b-1159fcb003df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'grouped_df', transforming 'NAME' to lowercase and dropping duplicates based on the 'name' column\n",
    "grouped_df_lower = grouped_df.select(lower(col(\"NAME\")).alias(\"name\")).dropDuplicates(['name'])\n",
    "\n",
    "# For 'df_flightdelay', transforming 'DEPARTING_AIRPORT' to lowercase, casting it to string, and dropping duplicates based on the 'departing_airport' column\n",
    "df_flightdelay_lower = df_flightdelay.select(lower(col(\"DEPARTING_AIRPORT\")).alias(\"departing_airport\")).dropDuplicates(['departing_airport'])\n",
    "\n",
    "\n",
    "\n",
    "#join providing table that contain in the name column all distinct airports \n",
    "#from weather dataset and under departing_flight all distinc airports from delay dataset\n",
    "\n",
    "#print(grouped_df_lower.count())\n",
    "#print(df_flightdelay_lower.count())\n",
    "result_df = df_flightdelay_lower.alias(\"flight\").join(\n",
    "    grouped_df_lower.alias(\"grouped\"),\n",
    "    (col(\"grouped.name\").contains(col(\"flight.departing_airport\"))),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"flight.departing_airport\").alias(\"departing_airport\"),\n",
    "    col(\"grouped.name\").alias(\"name\")\n",
    ")\n",
    "\n",
    "#result_df.show(n=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cad8c016-135e-4c98-83d9-f36c28a72c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modify dataframe such that df_result will contain the airports matched on join \n",
    "#and enhanced results will contain the df of unmatched airports for each dataset\n",
    "\n",
    "# Identifying non-matched entries\n",
    "non_matched_flight = df_flightdelay_lower.alias(\"flight\").join(\n",
    "    result_df.alias(\"result\"),\n",
    "    result_df.departing_airport == df_flightdelay_lower.departing_airport,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "non_matched_grouped = grouped_df_lower.alias(\"grouped\").join(\n",
    "    result_df.alias(\"result\"),\n",
    "    result_df.name == grouped_df_lower.name,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "\n",
    "# Add a unique ID to each DataFrame to facilitate the outer join\n",
    "result_df = result_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "non_matched_flight = non_matched_flight.withColumn(\"id\", monotonically_increasing_id())\n",
    "non_matched_grouped = non_matched_grouped.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "# Perform the outer joins using the unique IDs, result_df is now composed of matched airports\n",
    "enhanced_result_df = result_df.join(non_matched_flight, \"id\", \"outer\" ).join(non_matched_grouped, \"id\", \"outer\" )\n",
    "enhanced_result_df = enhanced_result_df.drop(\"id\")\n",
    "\n",
    "# Show the enhanced DataFrame with additional columns\n",
    "#enhanced_result_df.printSchema()\n",
    "\n",
    "# Select columns, get rid of duplicates\n",
    "selected_columns = [col for col in enhanced_result_df.columns if col != 'name' and col != 'departing_airport'] + ['grouped.name'] + ['flight.departing_airport']\n",
    "\n",
    "#will contain unmatched airports for each dataset\n",
    "enhanced_result_df = enhanced_result_df.select(selected_columns)\n",
    "enhanced_result_df.drop('name','departing_airport')\n",
    "#enhanced_result_df.show(n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a4742-f8c8-4707-97fe-808408ed0b5f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7419b18-8499-4eb5-b65b-3b70dda46365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dataframe that contains airports matched and unmatched result from the join\n",
    "# Rename columns in result_df\n",
    "result_df = result_df.withColumnRenamed(\"name\", \"weather_matched\") \\\n",
    "                     .withColumnRenamed(\"departing_airport\", \"delay_matched\")\n",
    "\n",
    "# Rename columns in enhanced_result_df\n",
    "enhanced_result_df = enhanced_result_df.withColumnRenamed(\"name\", \"weather_unmatched\") \\\n",
    "                                       .withColumnRenamed(\"departing_airport\", \"delay_unmatched\")\n",
    "\n",
    "# Optional: If you need to ensure the rows are matched by order, add an index column to each DataFrame\n",
    "result_df = result_df.withColumn(\"index\", monotonically_increasing_id())\n",
    "enhanced_result_df = enhanced_result_df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Join DataFrames on the index column\n",
    "matched_and_unmatched_airports = result_df.join(\n",
    "    enhanced_result_df,\n",
    "    on=\"index\",\n",
    "    how=\"outer\"  # Use \"outer\" to include all rows from both DataFrames\n",
    ")\n",
    "\n",
    "# Drop the index column as it's no longer needed after joining\n",
    "matched_and_unmatched_airports = matched_and_unmatched_airports.drop(\"index\", 'name', 'departing_airport')\n",
    "\n",
    "# Show the resulting DataFrame structure\n",
    "#matched_and_unmatched_airports.show(n = 76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e793e46b-e808-4fea-ad76-6a90cef070c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count the amount of airports for each clumn in the enhanced dataset dataframe \n",
    "\n",
    "non_null_name_count = result_df.filter(col(\"name\").isNotNull()).count()\n",
    "non_null_name_count1 = result_df.filter(col(\"departing_airport\").isNotNull()).count()\n",
    "non_null_name_count2 = enhanced_result_df.filter(col(\"weather_unmatched\").isNotNull()).count()\n",
    "non_null_name_count3 = enhanced_result_df.filter(col(\"delay_unmatched\").isNotNull()).count()\n",
    "#print(\"Number of non-null strings in the 'name' column:\", non_null_name_count, non_null_name_count1,non_null_name_count2, non_null_name_count3)\n",
    "\n",
    "# Display the filtered DataFrame and print the counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf47076e-52b6-4634-8e03-35ced09409df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the parsed data\n",
    "data = []\n",
    "\n",
    "# Open the text file and parse it line by line\n",
    "with open('./airports.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line by comma to extract the needed parts\n",
    "        parts = line.split(',')\n",
    "        \n",
    "        # Check if the line has enough parts to avoid index errors\n",
    "        if len(parts) >= 4:\n",
    "            # Extract and clean the desired parts\n",
    "            # Remove quotation marks and extra spaces if present\n",
    "            name = parts[1].strip('\"').strip()\n",
    "            city = parts[2].strip('\"').strip()\n",
    "            country = parts[3].strip('\"').strip()\n",
    "            \n",
    "            # Combine the first two parts into one column, and keep the country as the second column\n",
    "            combined = f\"{name}, {city}, {country}\"\n",
    "            data.append(combined)\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df_airports = pd.DataFrame(data, columns=['Airport and City'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f08a73d9-8f5b-4070-9e51-0d8af85f4a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fuzzywuzzy in /home/mate/.local/lib/python3.10/site-packages (0.18.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-Levenshtein in /home/mate/.local/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: Levenshtein==0.25.1 in /home/mate/.local/lib/python3.10/site-packages (from python-Levenshtein) (0.25.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in /home/mate/.local/lib/python3.10/site-packages (from Levenshtein==0.25.1->python-Levenshtein) (3.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c3c61f3-a54e-4edc-9d65-d4d2e84bf106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Get all airport names into one table\n",
    "df_delay_unique_airport = df_flightdelay.select('DEPARTING_AIRPORT').distinct().withColumnRenamed(\"DEPARTING_AIRPORT\", \"airport\")\n",
    "df_weather_unique_airport = df_weather.select('NAME').distinct().withColumnRenamed(\"NAME\", \"airport\")\n",
    "\n",
    "df_union = df_delay_unique_airport.union(df_weather_unique_airport)\n",
    "\n",
    "def filter_out_useless_parts_of_string(airport_name):\n",
    "    useless_words = [\"international\", \"airport\", \"regional\"]\n",
    "\n",
    "    modified = airport_name.lower()\n",
    "    for word in useless_words:\n",
    "        modified = modified.replace(word,\"\")\n",
    "        \n",
    "    return modified\n",
    "    \n",
    "filter_string_udf = udf(filter_out_useless_parts_of_string, StringType())\n",
    "\n",
    "df_union = df_union.select(filter_string_udf(col('airport'))).withColumnRenamed(\"filter_out_useless_parts_of_string(airport)\", \"airport\")\n",
    "#df_union.show(n=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8392322f-bd5d-47ea-ad10-763847e59700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "def get_matches(df1, col1, df2, col2, threshold=40):\n",
    "    # Convert each column to a list for processing, ensuring to drop NA values\n",
    "    list1 = df1[col1].dropna().tolist()\n",
    "    list2 = df2[col2].dropna().tolist()\n",
    "\n",
    "    # Find best matches with a score above the threshold\n",
    "    matches = []\n",
    "    for item in list1:\n",
    "        # Use process.extractOne to find the best match for each item from list1 in list2\n",
    "        best_match = process.extractOne(item, list2, scorer=fuzz.token_set_ratio)\n",
    "        if best_match and best_match[1] >= threshold:\n",
    "            matches.append((item, best_match[0], best_match[1]))\n",
    "\n",
    "    # Return matches as a DataFrame for better visualization\n",
    "    return pd.DataFrame(matches, columns=[col1, col2 + '_match', 'Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "935bd86e-6047-4e87-8d53-330b4cbd3419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add official airports using fuzzywuzzy\n",
    "# Assuming 'matched_and_unmatched_airports' is your PySpark DataFrame\n",
    "pandas_df = df_union.toPandas()  # Convert to Pandas DataFrame\n",
    "\n",
    "# Example usage (ensure df1 and df2 are already defined and loaded with your data)\n",
    "df_matches_airports = get_matches(pandas_df, 'airport', df_airports, 'Airport and City')\n",
    "\n",
    "spark_df_airports = spark.createDataFrame(df_matches_airports)\n",
    "# Show the DataFrame to verify conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5415602-ebd2-4bdd-a27b-13ec68b697a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joining_table = spark_df_airports.select(\"airport\", \"Airport and City_match\")\n",
    "#joining_table.show(n=2)\n",
    "delay_table = df_flightdelay.withColumn(\"DEPARTING_AIRPORT\", filter_string_udf('DEPARTING_AIRPORT'))\n",
    "#delay_table.show(n=2)\n",
    "weather_table = df_day_column.withColumn(\"NAME\", filter_string_udf('NAME'))\n",
    "#weather_table.show(n=2)\n",
    "\n",
    "#previous_arprt_table = df_flightdelay.withColumn(\"PREVIOUS_AIRPORT\", filter_string_udf('PREVIOUS_AIRPORT'))\n",
    "#previous_arprt_joined = previous_arprt_table.join(joining_table, joining_table.airport == previous_arprt_table.PREVIOUS_AIRPORT, 'inner')\n",
    "\n",
    "#delay_joined = delay_table.join(joining_table, joining_table.airport == delay_table.DEPARTING_AIRPORT, 'inner')\n",
    "#weather_joined = weather_table.join(joining_table, joining_table.airport == weather_table.NAME, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f51744-7a82-4551-aeb6-e5be6440d152",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fe908b1-39e2-44d4-8c86-5408294eb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#delay_joined.show(n=3)\n",
    "#weather_joined.show(n=3)\n",
    "\n",
    "# Convert integer columns in df1 to strings\n",
    "weather_joined = weather_joined.withColumn(\"MONTH\", col(\"MONTH\").cast(\"string\")) \\\n",
    "         .withColumn(\"DAY_OF_WEEK\", col(\"DAY_OF_WEEK\").cast(\"string\"))\n",
    "\n",
    "result_joined = delay_joined.join(weather_joined, [\"MONTH\",\"DAY_OF_WEEK\",\"Airport and City_match\"], 'inner')\n",
    "\n",
    "#result_joined.show(n=2)\n",
    "\n",
    "#weather_join_delay = weather_joined.join(delay_joined, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "610613f2-db54-482d-9caf-58d73ddcfda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e696e498-fb88-4ada-afbe-14974a82a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2c69994-b87c-4754-9200-b20cbcb50dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "# LATITUDE, LONGITUDE, STATION, MONTH, airport, normalized_name, NAME, _c0\n",
    "result_joined = result_joined.drop(\"LATITUDE\", \"LONGITUDE\", \"STATION\", \"MONTH\", \\\n",
    "                                   \"airport\", \"normalized_name\", \"NAME\", \"_c0\", \\\n",
    "                                   \"DATE\", \"AIRLINE_FLIGHTS_MONTH\", \"AVG_MONTHLY_PASS_AIRLINE\" \\\n",
    "                                   \"DEPARTING_AIRPORT\", \"PGTM\", \"WDF5\", \"WDF2\", \"WSF2\", \"WSF5\", \\\n",
    "                                   \"SN32\", \"SX32\", \"TOBS\",\"WESD\", \"PSUN\",\"TSUN\")\n",
    "\n",
    "df = result_joined\n",
    "#result_joined.count()\n",
    "#result_joined.select(\"CONCURRENT_FLIGHTS\").filter(col(\"CONCURRENT_FLIGHTS\").isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bc58762-a744-44fe-9c6e-8272a8e5e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"TMIN\", df[\"TMIN\"].cast(\"float\"))\n",
    "df = df.withColumn(\"PRCP\", df[\"PRCP\"].cast(\"float\"))\n",
    "\n",
    "\n",
    "#print(df.count())\n",
    "\n",
    "\n",
    "#count = df.filter(((col(\"PRCP\") > 0.0) & (col(\"TMIN\") < 3.0)) & (col(\"SNOW\").isNull() | (col(\"SNOW\") == ''))).count()\n",
    "\n",
    "df = df.withColumn(\"SNOW\", when(col(\"TMIN\") > 3, 0).otherwise(col(\"SNOW\")))\n",
    "df = df.withColumn(\"SNOW\", when( \\\n",
    "                    (col(\"SNOW\").isNull() | (col(\"SNOW\") == '')) & (col(\"PRCP\") > 0), col(\"PRCP\") \\\n",
    "                               ).otherwise(lit(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cfb6937-44ac-49c8-b956-12837d92fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify temperatures, create new flag column EXTREME_WEATHER based on TMIN and TMAX and drop all others\n",
    "\n",
    "\n",
    "# Since TMAX and TMIN are strings, you need to convert them to integers before comparison\n",
    "df = df.withColumn(\"TMAX\", df[\"TMAX\"].cast(\"integer\"))\n",
    "df = df.withColumn(\"TMIN\", df[\"TMIN\"].cast(\"integer\"))\n",
    "\n",
    "# Creating the EXTREME_WEATHER column based on the conditions provided\n",
    "df = df.withColumn(\"EXTREME_WEATHER\", \n",
    "                   when((col(\"TMAX\") > 40) | (col(\"TMIN\") < 0), 1)\n",
    "                   .otherwise(0))\n",
    "\n",
    "df = df.drop(\"TMIN\", \"TMAX\", \"TAVG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2395a6b-e573-4f21-bcc3-61898a3f3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Replace all WT** with column which adds these extreme weather conditions into one value\n",
    "values_as_strings = [f\"WT{i:02}\" for i in range(1, 12)]\n",
    "#print(values_as_strings)\n",
    "#print(\"+\".join(values_as_strings))\n",
    "\n",
    "for column_name in values_as_strings:\n",
    "    df = df.withColumn(column_name, df[column_name].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c36c5ab-63ce-4591-96e2-7780413559a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02f966b1-c944-4e85-bdc6-cfda4eede96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "total_wt_column = reduce(lambda a, b: a + b, [coalesce(col(c), lit(0)) for c in values_as_strings])\n",
    "\n",
    "df = df.withColumn('EXTREME_WEATHER_WT', total_wt_column)\n",
    "\n",
    "df = df.drop('WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT07', 'WT08', 'WT09', 'WT10', 'WT11')\n",
    "\n",
    "#df.printSchema()\n",
    "\n",
    "#df.groupBy(\"EXTREME_WEATHER_WT\").count().orderBy(\"EXTREME_WEATHER_WT\").show()\n",
    "#df.select(\"EXTREME_WEATHER_WT\").show(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16071707-3de0-41a5-80bb-f55cb3982655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CARRIER_NAME: string (nullable = true)\n",
      " |-- Airport and City_match: string (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- DEP_DEL15: string (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- SEGMENT_NUMBER: string (nullable = true)\n",
      " |-- CONCURRENT_FLIGHTS: string (nullable = true)\n",
      " |-- NUMBER_OF_SEATS: string (nullable = true)\n",
      " |-- AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AIRLINE_AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRPORT: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRLINE: string (nullable = true)\n",
      " |-- GROUND_SERV_PER_PASS: string (nullable = true)\n",
      " |-- PLANE_AGE: string (nullable = true)\n",
      " |-- DEPARTING_AIRPORT: string (nullable = true)\n",
      " |-- PREVIOUS_AIRPORT: string (nullable = true)\n",
      " |-- SNOW: float (nullable = true)\n",
      " |-- SNWD: string (nullable = true)\n",
      " |-- DATE_NEW: date (nullable = true)\n",
      " |-- EXTREME_WEATHER: integer (nullable = false)\n",
      " |-- EXTREME_WEATHER_WT: integer (nullable = false)\n",
      " |-- AWND: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- DISTANCE_GROUP: string (nullable = false)\n",
      " |-- FLT_ATTENDANTS_PER_PASS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7fdc75a-e3cc-481f-967d-469dff1dc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# Define a function to count nulls and empty strings\n",
    "def count_nulls_and_empties(df):\n",
    "    # Use aggregation to sum up each condition of being null or empty across all columns\n",
    "    exprs = [count(when(col(c).isNull() | (col(c) == \"\"), c)).alias(c) for c in df.columns]\n",
    "    return df.agg(*exprs)\n",
    "\n",
    "# Apply the function to count nulls and empties\n",
    "#nulls_and_empties_count = count_nulls_and_empties(df)\n",
    "\n",
    "# Show the result\n",
    "#nulls_and_empties_count.show()\n",
    "\n",
    "#df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a1a8912-5e86-4fcc-8fda-2702c487b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values, for example AWND and PRCP\n",
    "\n",
    "from pyspark.sql.functions import avg, col, coalesce, month, median\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window spec partitioned by month\n",
    "window_spec = Window.partitionBy(month(\"DATE_NEW\"))\n",
    "\n",
    "# Assuming 'column_name' is the column with null values you want to fill\n",
    "avg_column = avg(col(\"AWND\")).over(window_spec)\n",
    "avg_prcp = avg(col(\"PRCP\")).over(window_spec)\n",
    "med_column = median(col(\"FLT_ATTENDANTS_PER_PASS\")).over(window_spec)\n",
    "\n",
    "\n",
    "# Replace nulls with the average of that month\n",
    "df = df.withColumn(\"AWND_filled\", coalesce(col(\"AWND\"), avg_column))\n",
    "df = df.withColumn(\"PRCP_filled\", coalesce(col(\"PRCP\"), avg_prcp))\n",
    "df = df.withColumn(\"DISTANCE_GROUP_filled\", coalesce(col(\"DISTANCE_GROUP\"), lit(\"1\")))\n",
    "df = df.withColumn(\"FLT_ATTENDANTS_PER_PASS_filled\", coalesce(col(\"FLT_ATTENDANTS_PER_PASS\"), med_column))\n",
    "\n",
    "\n",
    "df = df.drop(\"AWND\").withColumnRenamed(\"AWND_filled\", \"AWND\")\n",
    "df = df.drop(\"PRCP\").withColumnRenamed(\"PRCP_filled\", \"PRCP\")\n",
    "df = df.drop(\"DISTANCE_GROUP\").withColumnRenamed(\"DISTANCE_GROUP_filled\", \"DISTANCE_GROUP\")\n",
    "df = df.drop(\"FLT_ATTENDANTS_PER_PASS\").withColumnRenamed(\"FLT_ATTENDANTS_PER_PASS_filled\", \"FLT_ATTENDANTS_PER_PASS\")\n",
    "\n",
    "#nulls_and_empties_count = count_nulls_and_empties(df)\n",
    "\n",
    "# Show the result\n",
    "#nulls_and_empties_count.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a075780-d721-40c3-9517-ccffafbd1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "calculate_flights = df.select(\"CARRIER_NAME\", \"Airport and City_match\", \"DATE_NEW\")\n",
    "\n",
    "calculate_flights = calculate_flights.withColumn(\"MONTH\", month(col(\"DATE_NEW\").alias(\"MONTH\")))\n",
    "\n",
    "count = calculate_flights.groupBy(\"CARRIER_NAME\", \"Airport and City_match\", \"MONTH\").count()\n",
    "#print(count.show())\n",
    "\n",
    "count = count.groupBy(\"CARRIER_NAME\", \"Airport and City_match\") \\\n",
    "                   .agg(round(avg(\"count\")).alias(\"monthly_avg_count\"))\n",
    "#print(count.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "237a0403-0565-4b6b-b853-bd1a77a38289",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsdf = df.join(\n",
    "    count,\n",
    "    [\"CARRIER_NAME\", \"Airport and City_match\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "rsdf = rsdf.withColumn(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\",\\\n",
    "                       when(\\\n",
    "                           (col(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\").isNull() | (col(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\") == ''))\n",
    "                           , col(\"monthly_avg_count\")).otherwise(col(\"AIRLINE_AIRPORT_FLIGHTS_MONTH\")))\n",
    "\n",
    "rsdf = rsdf.drop(\"monthly_avg_count\")\n",
    "\n",
    "#rsdf.show()\n",
    "\n",
    "result_df = result_joined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf93c537-7fbd-4dc7-85cf-9e0c887ec911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are any null values left\n",
    "df = rsdf\n",
    "\n",
    "#nulls_and_empties_count = count_nulls_and_empties(df)\n",
    "\n",
    "# Show the result\n",
    "#nulls_and_empties_count.show()\n",
    "#df.printSchema()\n",
    "#df.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01aa0cc6-5c29-41b8-91a1-da199cf4b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Converting from numerical to nominal\n",
    "# ------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cae857cd-5074-426f-9b12-da9a6f0e4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert precipitation from numerical to nominal\n",
    "\n",
    "# Calculate the quantile thresholds\n",
    "#thresholds = result_df.approxQuantile(\"PRCP\", [0.33, 0.67], 0.01)  # 0.01 is the relative error\n",
    "\n",
    "# Categorize based on quantile thresholds\n",
    "#result_df = result_df.withColumn(\n",
    "#    \"precip_category\",\n",
    "#    when(col(\"PRCP\") <= thresholds[0], \"low\")\n",
    "#    .when(col(\"PRCP\") <= thresholds[1], \"medium\")\n",
    "#    .otherwise(\"high\")\n",
    "#)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "#result_df.select(\"PRCP\", \"precip_category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0676654e-66b7-41ae-a093-f10e7832b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform weekday from numerical to nominal\n",
    "\n",
    "# Weekday mapping dictionary\n",
    "month_dict = {\n",
    "    '1': 'Monday', '2': 'Tuesday', '3': 'Wednesday', '4': 'Thursday', \n",
    "    '5': 'Friday', '6': 'Saturday', '7': 'Sunday'}\n",
    "\n",
    "# Define the UDF to convert numerical months to names\n",
    "def convert_weekday_to_name(weekday):\n",
    "    return month_dict.get(str(weekday), \"Unknown\")\n",
    "\n",
    "convert_weekday_udf = udf(convert_weekday_to_name, StringType())\n",
    "\n",
    "# Apply the UDF to create a new column with month names\n",
    "df_with_months = result_df.withColumn(\"DAY_OF_WEEK_NAME\", convert_weekday_udf(result_df[\"DAY_OF_WEEK\"]))\n",
    "#df_with_months.show(n=1)\n",
    "df_with_months = df_with_months.drop(\"DAY_OF_WEEK\").withColumnRenamed(\"DAY_OF_WEEK_NAME\", \"DAY_OF_WEEK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85d45649-18f7-41b7-bd6e-2ecee5b3df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER_OF_SEATS into nominal\n",
    "\n",
    "# Categorize based on research\n",
    "result_df = result_df.withColumn(\n",
    "    \"NUMBER_OF_SEATS_NOM\",\n",
    "    when(col(\"NUMBER_OF_SEATS\") <= 100, \"Small\")\n",
    "    .when(col(\"NUMBER_OF_SEATS\") <= 200, \"Medium\")\n",
    "    .when(col(\"NUMBER_OF_SEATS\") <= 400, \"Large\")\n",
    "    .otherwise(\"Jumbo\")\n",
    ")\n",
    "\n",
    "# Replace NUMBER_OF_SEATS column with the nominal one\n",
    "result_df = result_df.drop(\"NUMBER_OF_SEATS\").withColumnRenamed(\"NUMBER_OF_SEATS_NOM\", \"NUMBER_OF_SEATS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d43c8aee-bbab-4d38-a8d9-37cb1293e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plane age into nominal\n",
    "\n",
    "# Categorize based on research\n",
    "result_df = result_df.withColumn(\n",
    "    \"PLANE_AGE_NOM\",\n",
    "    when(col(\"PLANE_AGE\") <= 10, \"New\")\n",
    "    .when(col(\"PLANE_AGE\") <= 20, \"Standard\")\n",
    "    .otherwise(\"Old\")\n",
    ")\n",
    "\n",
    "# Replace PLANE_AGE column with the nominal one\n",
    "result_df = result_df.drop(\"PLANE_AGE\").withColumnRenamed(\"PLANE_AGE_NOM\", \"PLANE_AGE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2c735e4-60af-4f66-8712-aa54de234f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- Airport and City_match: string (nullable = true)\n",
      " |-- DEP_DEL15: string (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- DISTANCE_GROUP: string (nullable = true)\n",
      " |-- SEGMENT_NUMBER: string (nullable = true)\n",
      " |-- CONCURRENT_FLIGHTS: string (nullable = true)\n",
      " |-- CARRIER_NAME: string (nullable = true)\n",
      " |-- AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AIRLINE_AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRPORT: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRLINE: string (nullable = true)\n",
      " |-- FLT_ATTENDANTS_PER_PASS: string (nullable = true)\n",
      " |-- GROUND_SERV_PER_PASS: string (nullable = true)\n",
      " |-- DEPARTING_AIRPORT: string (nullable = true)\n",
      " |-- PREVIOUS_AIRPORT: string (nullable = true)\n",
      " |-- AWND: string (nullable = true)\n",
      " |-- PRCP: string (nullable = true)\n",
      " |-- SNOW: string (nullable = true)\n",
      " |-- SNWD: string (nullable = true)\n",
      " |-- TAVG: string (nullable = true)\n",
      " |-- TMAX: string (nullable = true)\n",
      " |-- TMIN: string (nullable = true)\n",
      " |-- WT01: string (nullable = true)\n",
      " |-- WT02: string (nullable = true)\n",
      " |-- WT03: string (nullable = true)\n",
      " |-- WT04: string (nullable = true)\n",
      " |-- WT05: string (nullable = true)\n",
      " |-- WT06: string (nullable = true)\n",
      " |-- WT07: string (nullable = true)\n",
      " |-- WT08: string (nullable = true)\n",
      " |-- WT09: string (nullable = true)\n",
      " |-- WT10: string (nullable = true)\n",
      " |-- WT11: string (nullable = true)\n",
      " |-- DATE_NEW: date (nullable = true)\n",
      " |-- NUMBER_OF_SEATS: string (nullable = false)\n",
      " |-- PLANE_AGE: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b8953a3-a05f-40c2-bbab-7bba3b16ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Data analysis\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e1396-fa90-47ce-a4f5-88af0c4c30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing batches\n",
    "fractions = {label: 0.1 for label in df.select(\"DEP_DEL15\").distinct().rdd.flatMap(lambda x: x).collect()}\n",
    "sampled_df = joined_df.stat.sampleBy(\"DEP_DEL15\", fractions, seed=1234)\n",
    "\n",
    "# Show the sampled data distribution\n",
    "sampled_df.groupBy(\"DEP_DEL15\").count().show()\n",
    "\n",
    "# Split the DataFrame into training (60%) and test (40%) sets\n",
    "train_df, test_df = sampled_df.randomSplit([0.6, 0.4], seed=1234)\n",
    "\n",
    "# Show the size of each set\n",
    "print(\"Training Dataset Count: \" + str(train_df.count()))\n",
    "print(\"Testing Dataset Count: \" + str(test_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e96b48-06b3-49be-9bb1-44039be849b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate staistics for numerical attributes\n",
    "numeric_columns = [\n",
    "    \"DAY_OF_WEEK\", \"DEP_DEL15\", \"SEGMENT_NUMBER\", \"CONCURRENT_FLIGHTS\",\n",
    "    \"NUMBER_OF_SEATS\", \"AIRPORT_FLIGHTS_MONTH\",\n",
    "    \"AIRLINE_AIRPORT_FLIGHTS_MONTH\", \"AVG_MONTHLY_PASS_AIRPORT\",\n",
    "    \"GROUND_SERV_PER_PASS\", \"SNOW\", \"EXTREME_WEATHER\", \"EXTREME_WEATHER_WT\",\n",
    "    \"AWND\", \"PRCP\", \"DISTANCE_GROUP\", \"FLT_ATTENDANTS_PER_PASS\"\n",
    "]\n",
    "\n",
    "# Cast each numeric column to float\n",
    "for column in numeric_columns:\n",
    "    df = df.withColumn(column, col(column).cast('integer'))\n",
    "    \n",
    "stats_df = df.describe()\n",
    "stats_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5669733-0662-4245-8d6c-92329b6d6ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 21:37:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:37:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:37:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:37:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:37:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:37:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:37:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:38:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:38:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:38:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:38:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:38:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:38:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:38:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:38:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:38:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:38:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:39:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:39:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:39:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:39:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:39:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:39:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:39:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:39:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:39:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:39:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:39:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 21:39:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|DEP_DEL15|  count|\n",
      "+---------+-------+\n",
      "|        0|1228130|\n",
      "|        1| 278647|\n",
      "+---------+-------+\n",
      "\n",
      "Original Distribution:\n",
      "SNOW: 0, Count: 12277239\n",
      "SNOW: 1, Count: 2774410\n",
      "\n",
      "Sampled Distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a 10% sample\n",
    "\n",
    "# Check distinct values in the DEP_DEL15 column and their distribution if needed\n",
    "snow_distribution = df.groupBy(\"DEP_DEL15\").count().collect()\n",
    "\n",
    "# Create a dictionary for fractions\n",
    "fractions = {row['DEP_DEL15']: 0.1 for row in snow_distribution}  # 10% sample for each category in SNOW\n",
    "\n",
    "# Sample the data maintaining the distribution of the SNOW column\n",
    "sampled_df = df.stat.sampleBy(\"DEP_DEL15\", fractions, seed=42)\n",
    "\n",
    "# Check the distribution in the sampled DataFrame\n",
    "sampled_distribution = sampled_df.groupBy(\"DEP_DEL15\").count().show()\n",
    "\n",
    "# Optionally, compare it to the original distribution\n",
    "print(\"Original Distribution:\")\n",
    "for row in snow_distribution:\n",
    "    print(\"SNOW: {}, Count: {}\".format(row['DEP_DEL15'], row['count']))\n",
    "\n",
    "print(\"\\nSampled Distribution:\")\n",
    "sampled_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7712cadd-37d4-41bb-bfc3-cb0cd7aa280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10% of dataset\n",
    "df = sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81c809-26e4-47ae-aa85-e2fb60f4d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # For better visual aesthetics\n",
    "\n",
    "# Setting a style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# List of nominal columns\n",
    "nominal_columns = [\n",
    "    \"CARRIER_NAME\",\n",
    "    \"Airport and City_match\",\n",
    "    \"DEP_TIME_BLK\",\n",
    "    \"DEPARTING_AIRPORT\",\n",
    "    \"PREVIOUS_AIRPORT\",\n",
    "    \"DATE_NEW\",\n",
    "    \"PLANE_AGE\"\n",
    "]\n",
    "\n",
    "# Create a figure to hold the subplots\n",
    "fig, axes = plt.subplots(nrows=len(nominal_columns), figsize=(12, 6 * len(nominal_columns)))  # Adjusted size for better fit\n",
    "\n",
    "# Loop through each nominal column and create a histogram\n",
    "for i, column in enumerate(nominal_columns):\n",
    "    # Count the occurrences of each category in the column\n",
    "    category_counts = df.groupBy(column).count().toPandas().sort_values(by='count', ascending=False)\n",
    "\n",
    "    # If there are many categories, you might want to limit the number of categories displayed\n",
    "    if len(category_counts) > 20:  # Adjust the threshold as needed\n",
    "        category_counts = category_counts.head(20)  # Only show top 20 categories\n",
    "    \n",
    "    # Plotting the histogram for the column\n",
    "    sns.barplot(x=column, y='count', data=category_counts, ax=axes[i], palette='viridis')  # Using seaborn for a better-looking plot\n",
    "    axes[i].set_title(f'Histogram of {column}')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Counts')\n",
    "    axes[i].tick_params(axis='x', rotation=45)  # Rotate labels to prevent overlap\n",
    "\n",
    "# Adjust layout to prevent overlap of subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f80505-7af2-462a-be76-ecc183bec053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate :  pomerového kritéria – informačného zisku voči cieľovému atribútu (klasifikačná úloha), pre nominálne atribúty \n",
    "from pyspark.sql.functions import col, log2, sum as sqlsum\n",
    "\n",
    "def entropy(df, column):\n",
    "    \"\"\"Calculate the entropy of a given DataFrame column.\"\"\"\n",
    "    # Calculate the total number of records in the DataFrame\n",
    "    total = df.count()\n",
    "    # Calculate the probabilities for each distinct value in the column\n",
    "    probabilities = df.groupBy(column).count().select((col('count') / total).alias('probability'))\n",
    "    # Calculate the entropy using the probability data\n",
    "    entropy = probabilities.withColumn('entropy_part', - col('probability') * log2(col('probability'))).agg(sqlsum('entropy_part')).collect()[0][0]\n",
    "    return entropy if entropy is not None else 0\n",
    "\n",
    "def information_gain(df, attribute, target):\n",
    "    \"\"\"Calculate the information gain of a column with respect to the target.\"\"\"\n",
    "    total_entropy = entropy(df, target)\n",
    "    total_count = df.count()\n",
    "\n",
    "    # Get distinct values of the attribute\n",
    "    attribute_values = df.select(attribute).distinct().collect()\n",
    "\n",
    "    conditional_entropy = 0\n",
    "    for row in attribute_values:\n",
    "        attr_val = row[attribute]\n",
    "        # Filter the DataFrame by each attribute value and calculate its entropy\n",
    "        df_subset = df.filter(col(attribute) == attr_val)\n",
    "        subset_count = df_subset.count()\n",
    "        if subset_count > 0:\n",
    "            # Compute entropy for each subset\n",
    "            subset_entropy = entropy(df_subset, target)\n",
    "            conditional_entropy += (subset_count / total_count) * subset_entropy\n",
    "\n",
    "    # Information gain is the total entropy minus the conditional entropy\n",
    "    return total_entropy - conditional_entropy\n",
    "\n",
    "# Example usage\n",
    "nominal_columns = [\"CARRIER_NAME\", \"Airport and City_match\", \"DEP_TIME_BLK\", \"DEPARTING_AIRPORT\", \"PREVIOUS_AIRPORT\", \"DATE_NEW\"]\n",
    "\n",
    "information_gains = {}\n",
    "for column in nominal_columns:\n",
    "    ig = information_gain(df, column, 'DEP_DEL15')  # Assuming DEP_DEL15 is your target column\n",
    "    information_gains[column] = ig\n",
    "    print(f'Information Gain for {column}: {ig}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1723b598-5ad4-44ba-ab28-a692d104c674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CARRIER_NAME: string (nullable = true)\n",
      " |-- Airport and City_match: string (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- DEP_DEL15: string (nullable = true)\n",
      " |-- DEP_TIME_BLK: string (nullable = true)\n",
      " |-- SEGMENT_NUMBER: string (nullable = true)\n",
      " |-- CONCURRENT_FLIGHTS: string (nullable = true)\n",
      " |-- NUMBER_OF_SEATS: string (nullable = true)\n",
      " |-- AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AIRLINE_AIRPORT_FLIGHTS_MONTH: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRPORT: string (nullable = true)\n",
      " |-- AVG_MONTHLY_PASS_AIRLINE: string (nullable = true)\n",
      " |-- GROUND_SERV_PER_PASS: string (nullable = true)\n",
      " |-- PLANE_AGE: string (nullable = true)\n",
      " |-- DEPARTING_AIRPORT: string (nullable = true)\n",
      " |-- PREVIOUS_AIRPORT: string (nullable = true)\n",
      " |-- SNOW: float (nullable = true)\n",
      " |-- SNWD: string (nullable = true)\n",
      " |-- DATE_NEW: date (nullable = true)\n",
      " |-- EXTREME_WEATHER: integer (nullable = false)\n",
      " |-- EXTREME_WEATHER_WT: integer (nullable = false)\n",
      " |-- AWND: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- DISTANCE_GROUP: string (nullable = false)\n",
      " |-- FLT_ATTENDANTS_PER_PASS: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe33646-1581-4c85-8187-63aab2482d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Model creation and training\n",
    "# ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089d2b5-1945-4bc7-9110-41e8bba691a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"DEPARTING_AIRPORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49816900-c826-435a-a3c3-c652208b9a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 22:28:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:28:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:34:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:34:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:39:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/04 22:40:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Plane age into numerical\n",
    "\n",
    "# Categorize based on research\n",
    "df = df.withColumn(\n",
    "    \"PLANE_AGE_NOM\",\n",
    "    when(col(\"PLANE_AGE\") == \"New\", 1)\n",
    "    .when(col(\"PLANE_AGE\") == \"Standard\", 2)\n",
    "    .otherwise(3)\n",
    ")\n",
    "\n",
    "# Replace PLANE_AGE column with the nominal one\n",
    "df = df.drop(\"PLANE_AGE\").withColumnRenamed(\"PLANE_AGE_NOM\", \"PLANE_AGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15198f-02e3-423a-95a3-1b9fbfc5d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER_OF_SEATS into numerical category\n",
    "\n",
    "# Categorize based on research\n",
    "result_df = result_df.withColumn(\n",
    "    \"NUMBER_OF_SEATS_NOM\",\n",
    "    when(col(\"NUMBER_OF_SEATS\") == \"Small\", 1)\n",
    "    .when(col(\"NUMBER_OF_SEATS\") == \"Medium\", 2)\n",
    "    .when(col(\"NUMBER_OF_SEATS\") == \"Large\", 3)\n",
    "    .otherwise(4)\n",
    ")\n",
    "\n",
    "# Replace NUMBER_OF_SEATS column with the nominal one\n",
    "result_df = result_df.drop(\"NUMBER_OF_SEATS\").withColumnRenamed(\"NUMBER_OF_SEATS_NOM\", \"NUMBER_OF_SEATS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979958c-96f1-4feb-8bcc-c5ac5bc30b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAY_OF_WEEK into numerical form\n",
    "df = df.withColumn(\"day_cos\", cos(radians(col(\"DAY_OF_WEEK\") * (360/7))))\n",
    "df = df.withColumn(\"day_sin\", sin(radians(col(\"DAY_OF_WEEK\") * (360/7))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb2fe7-c4e2-44cd-86ee-0c10059a2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beffd78e-acd4-4ca0-9b59-64a3592a34ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/05 16:31:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:31:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:50:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:50:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:50:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 16:56:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:02:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:02:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:08:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:08:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:14:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:14:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:19:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:19:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:26:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:31:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:31:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:37:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:37:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:42:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:42:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:48:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:48:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:53:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:53:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:53:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:54:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:59:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 17:59:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:04:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:04:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:10:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:10:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:15:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:15:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:21:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:22:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:27:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:27:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:33:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:33:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:38:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:38:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:43:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:43:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:52:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:52:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:52:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:52:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:52:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:52:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:52:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:53:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:53:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:53:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:53:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:53:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:53:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:53:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:53:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:53:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:53:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:53:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:53:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:53:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:58:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 18:58:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:04:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:04:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:09:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:09:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:15:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:15:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:21:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:22:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:22:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:22:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:22:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:22:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:22:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:22:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:22:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:22:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 19:22:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 6763:======================>                                (5 + 1) / 12]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.functions import col, sqrt, pow, mean, stddev\n",
    "\n",
    "\n",
    "# Convert necessary columns from string to appropriate numeric types\n",
    "columns_to_cast = [\"SEGMENT_NUMBER\", \"CONCURRENT_FLIGHTS\", \"NUMBER_OF_SEATS\", \n",
    "                   \"AIRPORT_FLIGHTS_MONTH\", \"AIRLINE_AIRPORT_FLIGHTS_MONTH\", \n",
    "                   \"AVG_MONTHLY_PASS_AIRPORT\", \"GROUND_SERV_PER_PASS\", \n",
    "                   \"SNOW\", \"AWND\", \"PRCP\", \"DISTANCE_GROUP\", \"FLT_ATTENDANTS_PER_PASS\"]\n",
    "\n",
    "for col_name in columns_to_cast:\n",
    "    df = df.withColumn(col_name, df[col_name].cast('float'))\n",
    "\n",
    "# Handle missing values if necessary, here using median or a predefined constant\n",
    "# Assuming missing values handled outside of this snippet or need to be addressed\n",
    "# df = df.fillna({'someColumn': value})\n",
    "\n",
    "# Assemble features, excluding the target 'DEP_DEL15'\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"SEGMENT_NUMBER\", \"CONCURRENT_FLIGHTS\", \"NUMBER_OF_SEATS\", \"AIRPORT_FLIGHTS_MONTH\",\n",
    "               \"AIRLINE_AIRPORT_FLIGHTS_MONTH\", \"AVG_MONTHLY_PASS_AIRPORT\", \"GROUND_SERV_PER_PASS\",\n",
    "               \"SNOW\", \"AWND\", \"PRCP\", \"DISTANCE_GROUP\", \"FLT_ATTENDANTS_PER_PASS\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "featured_df = assembler.transform(df)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "scaled_df = scaler.fit(featured_df).transform(featured_df)\n",
    "\n",
    "# Determine the optimal number of clusters using the Silhouette Score\n",
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='scaledFeatures', metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "silhouette_scores = []\n",
    "for k in range(2, 10):  # Adjust the range of k as needed\n",
    "    kmeans = KMeans(featuresCol=\"scaledFeatures\", k=k)\n",
    "    model = kmeans.fit(scaled_df)\n",
    "    predictions = model.transform(scaled_df)\n",
    "    score = evaluator.evaluate(predictions)\n",
    "    silhouette_scores.append((k, score))\n",
    "\n",
    "# Select the best k with the highest silhouette score\n",
    "best_k = max(silhouette_scores, key=lambda item: item[1])[0]\n",
    "\n",
    "# Build the final KMeans model with the best k\n",
    "final_kmeans = KMeans(featuresCol=\"scaledFeatures\", k=best_k)\n",
    "final_model = final_kmeans.fit(scaled_df)\n",
    "final_predictions = final_model.transform(scaled_df)\n",
    "\n",
    "# Calculate distances to cluster center for anomaly detection\n",
    "centers = final_model.clusterCenters()\n",
    "get_distance = lambda features, center: sqrt(sum([pow((f - c), 2) for f, c in zip(features, center)]))\n",
    "udf_get_distance = F.udf(get_distance, FloatType())\n",
    "\n",
    "# Adding a column for distances to each center\n",
    "for i, center in enumerate(centers):\n",
    "    final_predictions = final_predictions.withColumn(f\"dist_to_center_{i}\", udf_get_distance(col(\"scaledFeatures\"), F.array([F.lit(x) for x in center])))\n",
    "\n",
    "# Setting anomaly detection threshold\n",
    "distance_desc = final_predictions.describe([\"dist_to_center_0\"])  # Assuming using distance to center of first cluster\n",
    "mean_distance = float(distance_desc.filter(\"summary = 'mean'\").select(\"dist_to_center_0\").collect()[0][0])\n",
    "stddev_distance = float(distance_desc.filter(\"summary = 'stddev'\").select(\"dist_to_center_0\").collect()[0][0])\n",
    "threshold = mean_distance + 3 * stddev_distance\n",
    "anomalies = final_predictions.filter(final_predictions[\"dist_to_center_0\"] > threshold)\n",
    "\n",
    "# Show results\n",
    "anomalies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34276217-f159-4dd6-89a2-aca3589ec084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1866:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineModel_63b22420a9e9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Indexing and Encoding for 'CARRIER_NAME'\n",
    "carrier_name_indexer = StringIndexer(inputCol=\"CARRIER_NAME\", outputCol=\"CARRIER_NAME_Index\")\n",
    "carrier_name_encoder = OneHotEncoder(inputCol=\"CARRIER_NAME_Index\", outputCol=\"CARRIER_NAME_Encoded\")\n",
    "\n",
    "# Indexing and Encoding for 'PREVIOUS_AIRPORT'\n",
    "prev_airport_indexer = StringIndexer(inputCol=\"PREVIOUS_AIRPORT\", outputCol=\"PREVIOUS_AIRPORT_Index\")\n",
    "prev_airport_encoder = OneHotEncoder(inputCol=\"PREVIOUS_AIRPORT_Index\", outputCol=\"PREVIOUS_AIRPORT_Encoded\")\n",
    "\n",
    "# Indexing and Encoding for 'Airport and City_match'\n",
    "city_match_indexer = StringIndexer(inputCol=\"Airport and City_match\", outputCol=\"Airport_and_City_match_Index\")\n",
    "city_match_encoder = OneHotEncoder(inputCol=\"Airport_and_City_match_Index\", outputCol=\"Airport_and_City_match_Encoded\")\n",
    "\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"PREVIOUS_AIRPORT_Encoded\", \"Airport_and_City_match_Encoded\", \"CARRIER_NAME_Encoded\"] + [\"SNOW\"],  # Replace \"other_features\" with actual feature names\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Pipeline all preprocessing steps\n",
    "pipeline = Pipeline(stages=[\n",
    "    prev_airport_indexer, prev_airport_encoder, city_match_indexer, city_match_encoder, carrier_name_indexer, carrier_name_encoder, assembler\n",
    "])\n",
    "\n",
    "# Fit and Transform\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# Unclear if works correctly\n",
    "#transformed_df = model.transform(df)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca4b422-7f9e-450d-9436-cc4206df05c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### AT THIS POINT DEFINE train_data AND test_data\n",
    "# Like so for example: train_data, test_data = final_data.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89e15029-e279-406b-9451-39fa03ca6482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1965:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+-----------+---------+------------+--------------+------------------+---------------+---------------------+-----------------------------+------------------------+------------------------+--------------------+---------+-----------------+--------------------+----+----+----------+---------------+------------------+-----+----+--------------+-----------------------+----------------------+------------------------+----------------------------+------------------------------+------------------+--------------------+--------------------+\n",
      "|        CARRIER_NAME|Airport and City_match|DAY_OF_WEEK|DEP_DEL15|DEP_TIME_BLK|SEGMENT_NUMBER|CONCURRENT_FLIGHTS|NUMBER_OF_SEATS|AIRPORT_FLIGHTS_MONTH|AIRLINE_AIRPORT_FLIGHTS_MONTH|AVG_MONTHLY_PASS_AIRPORT|AVG_MONTHLY_PASS_AIRLINE|GROUND_SERV_PER_PASS|PLANE_AGE|DEPARTING_AIRPORT|    PREVIOUS_AIRPORT|SNOW|SNWD|  DATE_NEW|EXTREME_WEATHER|EXTREME_WEATHER_WT| AWND|PRCP|DISTANCE_GROUP|FLT_ATTENDANTS_PER_PASS|PREVIOUS_AIRPORT_Index|PREVIOUS_AIRPORT_Encoded|Airport_and_City_match_Index|Airport_and_City_match_Encoded|CARRIER_NAME_Index|CARRIER_NAME_Encoded|            features|\n",
      "+--------------------+----------------------+-----------+---------+------------+--------------+------------------+---------------+---------------------+-----------------------------+------------------------+------------------------+--------------------+---------+-----------------+--------------------+----+----+----------+---------------+------------------+-----+----+--------------+-----------------------+----------------------+------------------------+----------------------------+------------------------------+------------------+--------------------+--------------------+\n",
      "|Alaska Airlines Inc.|  Dallas Love Field...|          5|        0|   1400-1459|           2.0|               8.0|          120.0|               6204.0|                         31.0|                673221.0|                 2884187|        1.7460145E-4|       20|dallas love field|Seattle Internati...| 0.0| 0.0|2019-07-04|              1|                 1|12.08|0.14|           1.0|            3.233146E-5|                  14.0|        (294,[14],[1.0])|                        16.0|               (51,[16],[1.0])|               5.0|      (16,[5],[1.0])|(362,[14,310,350]...|\n",
      "|Alaska Airlines Inc.|  Dallas Love Field...|          7|        0|   1400-1459|           3.0|              14.0|          181.0|               5891.0|                         30.0|                673221.0|                 2884187|        1.7460145E-4|        2|dallas love field|Seattle Internati...| 0.0| 0.0|2019-06-29|              1|                 1| 5.37|0.02|           7.0|            3.233146E-5|                  14.0|        (294,[14],[1.0])|                        16.0|               (51,[16],[1.0])|               5.0|      (16,[5],[1.0])|(362,[14,310,350]...|\n",
      "+--------------------+----------------------+-----------+---------+------------+--------------+------------------+---------------+---------------------+-----------------------------+------------------------+------------------------+--------------------+---------+-----------------+--------------------+----+----+----------+---------------+------------------+-----+----+--------------+-----------------------+----------------------+------------------------+----------------------------+------------------------------+------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/05 00:06:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/05 00:06:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transformed_df = model.transform(df)\n",
    "transformed_df.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93845994-30e5-453c-a8d2-3c65ee06984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes model\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "# Indexing string columns to convert to numeric values for model input\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df)\n",
    "    for column in list(set(df.columns) - set(['DEP_DEL15'])) # Assuming 'DEP_DEL15' is the label\n",
    "]\n",
    "\n",
    "# Use Pipeline to apply indexers\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_r = pipeline.fit(df).transform(df)\n",
    "\n",
    "# VectorAssembler to create feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[column + \"_index\" for column in list(set(df.columns) - set(['DEP_DEL15']))],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(df_r)\n",
    "\n",
    "# Select features and label for model training\n",
    "final_data = data.select(col(\"features\"), col(\"DEP_DEL15\").alias(\"label\"))\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# Train the model\n",
    "model = nb.fit(train_data)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76577fe5-c96a-49db-bce4-38465beb7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Initialize the LinearSVC model\n",
    "\n",
    "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=100, regParam=0.1)\n",
    "\n",
    "# Fit the model on training data\n",
    "svm_model = svm.fit(train_data)\n",
    "\n",
    "# Predictions\n",
    "predictions = svm_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Area under ROC: {auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054460c-2b7e-410c-b3d2-0136b05f9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10)\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g\" % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
