{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "118939be-42d4-4a27-b4df-758358c8e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_date, concat, lit\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/hel/.local/lib/python3.10/site-packages/pyspark/\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628aa5bc-3116-432b-836b-b7180bf82f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/28 18:12:03 WARN Utils: Your hostname, hel-ThinkPad-T14-Gen-3 resolves to a loopback address: 127.0.1.1; using 192.168.1.24 instead (on interface wlp2s0)\n",
      "24/04/28 18:12:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/28 18:12:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"My Spark Application\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "109dc1f4-2b67-4b28-9e07-abced251b380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| ID|Value|\n",
      "+---+-----+\n",
      "|  1|  foo|\n",
      "|  2|  bar|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a DataFrame and showing its content\n",
    "df = spark.createDataFrame([(1, 'foo'), (2, 'bar')], [\"ID\", \"Value\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "01556bb0-76d9-41a3-8586-e6275b034fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+--------+-----+-----+----+----+----+----+----+----+----+-----+\n",
      "|    STATION|                NAME|DAY_OF_WEEK|    DATE|MONTH| AWND|PGTM|PRCP|SNOW|SNWD|TAVG|TMAX|TMIN| WDF2|\n",
      "+-----------+--------------------+-----------+--------+-----+-----+----+----+----+----+----+----+----+-----+\n",
      "|USW00013874|ATLANTA HARTSFIEL...|          3|1/1/2019|    1|  4.7|NULL|0.14| 0.0| 0.0|64.0|66.0|57.0|310.0|\n",
      "|USW00013874|ATLANTA HARTSFIEL...|          4|1/2/2019|    1| 4.92|NULL|0.57| 0.0| 0.0|56.0|59.0|49.0| 70.0|\n",
      "|USW00013874|ATLANTA HARTSFIEL...|          5|1/3/2019|    1| 5.37|NULL|0.15| 0.0| 0.0|52.0|55.0|51.0|340.0|\n",
      "|USW00013874|ATLANTA HARTSFIEL...|          6|1/4/2019|    1|12.08|NULL|1.44| 0.0| 0.0|56.0|66.0|45.0|260.0|\n",
      "|USW00013874|ATLANTA HARTSFIEL...|          7|1/5/2019|    1|13.42|NULL| 0.0| 0.0| 0.0|49.0|59.0|44.0|280.0|\n",
      "+-----------+--------------------+-----------+--------+-----+-----+----+----+----+----+----+----+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+------------------+------------------+\n",
      "|                NAME|MONTH|              AWND|         avg(PGTM)|\n",
      "+--------------------+-----+------------------+------------------+\n",
      "|ALBANY INTERNATIO...| NULL| 7.536356164383563|              NULL|\n",
      "|ALBUQUERQUE INTER...| NULL|  8.26309589041096|          1740.125|\n",
      "|ANCHORAGE TED STE...| NULL| 6.412356164383568|1224.4875346260387|\n",
      "|ASHEVILLE AIRPORT...| NULL| 6.157005494505495|              NULL|\n",
      "|ASPEN PITKIN CO A...| NULL| 6.166465753424659|1454.6451612903227|\n",
      "|ATLANTA HARTSFIEL...|    1| 9.070645161290322|              NULL|\n",
      "|ATLANTA HARTSFIEL...|    2| 8.619642857142855|              NULL|\n",
      "|ATLANTA HARTSFIEL...|    3| 9.243870967741938|              NULL|\n",
      "|ATLANTA HARTSFIEL...|    4| 8.008000000000001|              NULL|\n",
      "|ATLANTA HARTSFIEL...|    5| 6.803870967741937|              NULL|\n",
      "|ATLANTA HARTSFIEL...|    6|7.3823333333333325|              NULL|\n",
      "|ATLANTA HARTSFIEL...|    7| 6.515161290322582|1483.3548387096773|\n",
      "|ATLANTA HARTSFIEL...|    8| 6.631612903225806|              NULL|\n",
      "|ATLANTA HARTSFIEL...|    9| 6.845666666666669|              NULL|\n",
      "|ATLANTA HARTSFIEL...|   10| 8.399032258064516|              NULL|\n",
      "|ATLANTA HARTSFIEL...|   11| 7.970666666666665|              NULL|\n",
      "|ATLANTA HARTSFIEL...|   12|10.087741935483868|              NULL|\n",
      "|AUSTIN BERGSTROM ...|    1| 7.699677419354839|              NULL|\n",
      "|AUSTIN BERGSTROM ...|    2| 9.282857142857143|              NULL|\n",
      "|AUSTIN BERGSTROM ...|    3| 9.987741935483873|              NULL|\n",
      "+--------------------+-----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, dayofweek,to_date, month, count, avg\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number,   sum, when\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path_flightdelay = \"./full_data_flightdelay.csv\"  # Replace with the path to your CSV file\n",
    "\n",
    "\n",
    "df_flightdelay = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(csv_file_path_flightdelay)\n",
    "\n",
    "\n",
    "# Read the CSV file using the manually defined schema\n",
    "csv_file_path_weather = \"./airport_weather_2019.csv\"  # Replace with your file path\n",
    "df_weather = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(csv_file_path_weather)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ab71331e-dd5a-4fe4-96d8-971c711c5836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38675\n"
     ]
    }
   ],
   "source": [
    "print(df_weather.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0c09dbf2-39ce-42c3-9e35-35ffc9c47fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+--------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "|MONTH|                NAME|              AWND|                PRCP|               SNOW|               SNWD|              TAVG|              TMAX|              TMIN|              WDF2|\n",
      "+-----+--------------------+------------------+--------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "|    1|ALBANY INTERNATIO...| 9.604516129032257| 0.13838709677419353| 0.6096774193548388| 2.5064516129032257|23.967741935483872| 31.93548387096774|14.709677419354838|231.29032258064515|\n",
      "|    2|ALBANY INTERNATIO...| 8.803214285714287| 0.09571428571428572| 0.5035714285714284|              2.525|27.857142857142858|35.392857142857146|             19.75|233.57142857142858|\n",
      "|    3|ALBANY INTERNATIO...| 9.698064516129032|0.045000000000000005|0.21290322580645163|  1.348387096774194|35.354838709677416| 44.58064516129032| 25.70967741935484|248.06451612903226|\n",
      "|    4|ALBANY INTERNATIO...| 9.573666666666668|               0.144|               0.07|               0.04|50.166666666666664| 59.93333333333333| 39.63333333333333|             234.0|\n",
      "|    5|ALBANY INTERNATIO...| 7.662903225806452|  0.0993548387096774|                0.0|                0.0|58.096774193548384| 67.93548387096774| 49.12903225806452|210.96774193548387|\n",
      "|    6|ALBANY INTERNATIO...| 6.658999999999998| 0.17166666666666666|                0.0|                0.0| 68.53333333333333| 78.93333333333334| 57.06666666666667|229.66666666666666|\n",
      "|    7|ALBANY INTERNATIO...|5.3835483870967735| 0.12870967741935482|                0.0|                0.0| 75.96774193548387|  86.6774193548387|  65.6774193548387|211.93548387096774|\n",
      "|    8|ALBANY INTERNATIO...| 5.613870967741936|  0.1329032258064516|                0.0|                0.0|  71.2258064516129|  82.2258064516129| 61.16129032258065| 216.1290322580645|\n",
      "|    9|ALBANY INTERNATIO...| 5.965333333333336| 0.07433333333333333|                0.0|                0.0| 64.06666666666666|              75.2|              52.4|197.66666666666666|\n",
      "|   10|ALBANY INTERNATIO...| 6.400967741935484| 0.24129032258064514|                0.0|                0.0|53.774193548387096| 62.74193548387097| 44.41935483870968|212.58064516129033|\n",
      "|   11|ALBANY INTERNATIO...| 8.060333333333332|               0.094|0.07666666666666667|               0.16| 37.13333333333333|44.733333333333334|              28.7|214.33333333333334|\n",
      "|   12|ALBANY INTERNATIO...| 7.137096774193547| 0.14645161290322578| 0.9000000000000001|  3.761290322580645|30.483870967741936|36.903225806451616|22.677419354838708|225.16129032258064|\n",
      "|    1|ALBUQUERQUE INTER...| 6.725161290322581|0.016774193548387096|0.06451612903225806|0.12903225806451613| 35.32258064516129| 46.03225806451613|25.806451612903224|  257.741935483871|\n",
      "|    2|ALBUQUERQUE INTER...| 9.251428571428573|0.017857142857142856|               0.15|0.11071428571428572|39.285714285714285|              51.5|28.321428571428573|239.64285714285714|\n",
      "|    3|ALBUQUERQUE INTER...| 10.17451612903226|0.023548387096774197|                0.0|                0.0| 50.29032258064516| 62.16129032258065|38.225806451612904|217.09677419354838|\n",
      "|    4|ALBUQUERQUE INTER...| 8.835666666666665|0.033666666666666664|                0.0|                0.0|              57.6| 70.26666666666667| 44.43333333333333|             215.0|\n",
      "|    5|ALBUQUERQUE INTER...|  9.77741935483871|0.007741935483870969|                0.0|                0.0| 61.70967741935484|              75.0|48.193548387096776|217.74193548387098|\n",
      "|    6|ALBUQUERQUE INTER...| 9.857333333333333|0.002333333333333...|                0.0|                0.0|              74.4|              87.9|              59.7|205.66666666666666|\n",
      "|    7|ALBUQUERQUE INTER...|  8.42032258064516| 0.06290322580645161|                0.0|                0.0| 80.03225806451613| 93.48387096774194| 67.90322580645162| 163.2258064516129|\n",
      "|    8|ALBUQUERQUE INTER...|7.2377419354838715|0.013870967741935483|                0.0|                0.0| 78.96774193548387|  93.2258064516129| 66.03225806451613|192.25806451612902|\n",
      "+-----+--------------------+------------------+--------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# create new column for month and day_of_week values derived from date\n",
    "df_day_column = df_weather.withColumn(\"DATE_NEW\", to_date(col(\"DATE\"), \"M/d/yyyy\"))\n",
    "df_day_column = df_day_column.withColumn(\"DATE_NEW\", coalesce(df_day_column[\"DATE_NEW\"], to_date(df_day_column[\"DATE\"], 'yyyy-MM-dd')))\n",
    "    \n",
    "df_day_column = df_day_column.withColumn(\"DAY_OF_WEEK\", dayofweek(col(\"DATE_NEW\").alias(\"DAY_OF_WEEK\")))\n",
    "df_day_column = df_day_column.withColumn(\"MONTH\", month(col(\"DATE_NEW\").alias(\"MONTH\")))\n",
    "#df_day_column = df_weather.withColumn(\"DAY_OF_WEEK\", dayofweek(col(\"DATE\").alias(\"DAY_OF_WEEK\"))) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df_day_column.show(n=2)\n",
    "df_day_column.createOrReplaceTempView(\"table1\")\n",
    "df_select = spark.sql(\"SELECT STATION, NAME,DAY_OF_WEEK,DATE, MONTH, AWND, PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, WDF2 from table1\")\n",
    "#df_select.show(n=5)\n",
    "\n",
    "grouped_df = df_select.groupBy(\"MONTH\", \"NAME\").agg(\n",
    "    avg(\"AWND\").alias(\"AWND\"),\n",
    "    avg(\"PRCP\").alias(\"PRCP\"),\n",
    "    avg(\"SNOW\").alias(\"SNOW\"),\n",
    "    avg(\"SNWD\").alias(\"SNWD\"),\n",
    "    avg(\"TAVG\").alias(\"TAVG\"),\n",
    "    avg(\"TMAX\").alias(\"TMAX\"),\n",
    "    avg(\"TMIN\").alias(\"TMIN\"),\n",
    "    avg(\"WDF2\").alias(\"WDF2\")\n",
    ").orderBy(\"NAME\",\"MONTH\")\n",
    "\n",
    "\n",
    "grouped_df.show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f2df1-13be-42d4-8455-7a1d74bd8985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec282309-b8bf-45cf-a80b-a23cbbd644b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "66837d93-05eb-4042-bf6b-1159fcb003df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "106\n",
      "+-----+--------------------+-----------------+--------------------+--------------------+----+-----------------+-----------------+------------------+------------------+--------------------+\n",
      "|MONTH|                NAME|             AWND|                PRCP|                SNOW|SNWD|             TAVG|             TMAX|              TMIN|              WDF2|     normalized_name|\n",
      "+-----+--------------------+-----------------+--------------------+--------------------+----+-----------------+-----------------+------------------+------------------+--------------------+\n",
      "|    9|COLORADO SPRINGS ...|9.975999999999999|0.010666666666666668|                 0.0| 0.0|68.53333333333333|84.03333333333333|53.333333333333336|190.33333333333334|colorado springs ...|\n",
      "|   11|SPRINGFIELD WEATH...|9.499333333333334| 0.10966666666666666|0.013333333333333334| 0.0|             42.6|             55.0|31.766666666666666|             223.0|springfield weath...|\n",
      "+-----+--------------------+-----------------+--------------------+--------------------+----+-----------------+-----------------+------------------+------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 915:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+---------+------------+--------------+--------------+------------------+---------------+--------------------+---------------------+---------------------+-----------------------------+------------------------+------------------------+-----------------------+--------------------+---------+--------------------+--------+---------+--------------------+---------------+\n",
      "|MONTH|DAY_OF_WEEK|DEP_DEL15|DEP_TIME_BLK|DISTANCE_GROUP|SEGMENT_NUMBER|CONCURRENT_FLIGHTS|NUMBER_OF_SEATS|        CARRIER_NAME|AIRPORT_FLIGHTS_MONTH|AIRLINE_FLIGHTS_MONTH|AIRLINE_AIRPORT_FLIGHTS_MONTH|AVG_MONTHLY_PASS_AIRPORT|AVG_MONTHLY_PASS_AIRLINE|FLT_ATTENDANTS_PER_PASS|GROUND_SERV_PER_PASS|PLANE_AGE|   DEPARTING_AIRPORT|LATITUDE|LONGITUDE|    PREVIOUS_AIRPORT|normalized_name|\n",
      "+-----+-----------+---------+------------+--------------+--------------+------------------+---------------+--------------------+---------------------+---------------------+-----------------------------+------------------------+------------------------+-----------------------+--------------------+---------+--------------------+--------+---------+--------------------+---------------+\n",
      "|    1|          3|        0|   1100-1159|             3|             3|                 2|             50|SkyWest Airlines ...|                 1561|                62105|                          869|                  171473|                 3472966|   3.41926740144364E-05|9.90027880586417E-05|       16|  Boise Air Terminal|  43.565| -116.225|Stapleton Interna...|          boise|\n",
      "|    1|          5|        0|   1800-1859|             1|             5|                13|            128|American Airlines...|                 4677|                75506|                          726|                  573075|                11744595|   9.82082928995461E-05|   0.000177287219593|       19|Raleigh-Durham In...|  35.875|  -78.782|   Douglas Municipal| raleigh-durham|\n",
      "+-----+-----------+---------+------------+--------------+--------------+------------------+---------------+--------------------+---------------------+---------------------+-----------------------------+------------------------+------------------------+-----------------------+--------------------+---------+--------------------+--------+---------+--------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, split\n",
    "\n",
    "# Normalize joining columns\n",
    "grouped_df = grouped_df.withColumn(\"normalized_name\", lower(col(\"name\")))\n",
    "df_flightdelay = df_flightdelay.withColumn(\"normalized_name\", lower(split(col(\"departing_airport\"), \" \").getItem(0)))\n",
    "\n",
    "# Group by to investigate\n",
    "grouped_df_nn = grouped_df.groupBy(\"normalized_name\").agg(\n",
    "    count('*').alias('count')\n",
    ")\n",
    "\n",
    "grouped_df_name = grouped_df.groupBy(\"NAME\").agg(\n",
    "    count('*').alias('count')\n",
    ")\n",
    "\n",
    "print(grouped_df_nn.count())\n",
    "print(grouped_df_name.count())\n",
    "\n",
    "# Only unique values\n",
    "grouped_df = grouped_df.distinct()\n",
    "df_flightdelay = df_flightdelay.distinct()\n",
    "\n",
    "\n",
    "# Join dataframes grouped_df and df_flightdelay\n",
    "grouped_df.show(n=2)\n",
    "df_flightdelay.show(n=2)\n",
    "joined_df = df_flightdelay.alias('f').join(\n",
    "    grouped_df.alias('g'),\n",
    "    (col('g.month') == col('f.month')) & (col('g.normalized_name')).contains(col('f.normalized_name')), 'inner'\n",
    ")\n",
    "\n",
    "#joined_df.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a5f7dc5e-1efd-4a5b-9d0c-cccea01c9bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1830444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 854:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(joined_df.count())\n",
    "print(df_flightdelay.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "172f429f-79b7-4500-95e5-69adc835f84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|DEP_DEL15| count|\n",
      "+---------+------+\n",
      "|        0|147142|\n",
      "|        1| 36074|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 109967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 790:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Dataset Count: 73249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "fractions = {label: 0.1 for label in joined_df.select(\"DEP_DEL15\").distinct().rdd.flatMap(lambda x: x).collect()}\n",
    "sampled_df = joined_df.stat.sampleBy(\"DEP_DEL15\", fractions, seed=1234)\n",
    "\n",
    "# Show the sampled data distribution\n",
    "sampled_df.groupBy(\"DEP_DEL15\").count().show()\n",
    "\n",
    "# Split the DataFrame into training (60%) and test (40%) sets\n",
    "train_df, test_df = sampled_df.randomSplit([0.6, 0.4], seed=1234)\n",
    "\n",
    "# Show the size of each set\n",
    "print(\"Training Dataset Count: \" + str(train_df.count()))\n",
    "print(\"Testing Dataset Count: \" + str(test_df.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5c03660f-4c5e-4e00-a7fd-2e9c5457d10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 805:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|                PRCP|precip_category|\n",
      "+--------------------+---------------+\n",
      "| 0.03483870967741936|            low|\n",
      "| 0.05741935483870968|            low|\n",
      "|  0.1235483870967742|         medium|\n",
      "|  0.1235483870967742|         medium|\n",
      "| 0.06322580645161291|            low|\n",
      "| 0.06451612903225806|            low|\n",
      "| 0.06451612903225806|            low|\n",
      "| 0.13580645161290322|           high|\n",
      "|  0.0932258064516129|         medium|\n",
      "| 0.06774193548387097|            low|\n",
      "| 0.09032258064516129|         medium|\n",
      "| 0.16225806451612904|           high|\n",
      "|0.052580645161290324|            low|\n",
      "| 0.14741935483870963|           high|\n",
      "|0.040322580645161296|            low|\n",
      "| 0.13580645161290322|           high|\n",
      "| 0.20096774193548386|           high|\n",
      "|0.040322580645161296|            low|\n",
      "| 0.13580645161290322|           high|\n",
      "| 0.20096774193548386|           high|\n",
      "+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#numerical to nominal\n",
    "# Calculate the quantile thresholds\n",
    "thresholds = joined_df.approxQuantile(\"PRCP\", [0.33, 0.67], 0.01)  # 0.01 is the relative error\n",
    "\n",
    "# Categorize based on quantile thresholds\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"precip_category\",\n",
    "    when(col(\"PRCP\") <= thresholds[0], \"low\")\n",
    "    .when(col(\"PRCP\") <= thresholds[1], \"medium\")\n",
    "    .otherwise(\"high\")\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "joined_df.select(\"PRCP\", \"precip_category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69caaf58-3343-4759-9f6b-3df108905834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
