{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118939be-42d4-4a27-b4df-758358c8e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_date, concat, lit\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/hel/.local/lib/python3.10/site-packages/pyspark/\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628aa5bc-3116-432b-836b-b7180bf82f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/03 13:08:34 WARN Utils: Your hostname, hel-ThinkPad-T14-Gen-3 resolves to a loopback address: 127.0.1.1; using 192.168.1.24 instead (on interface wlp2s0)\n",
      "24/05/03 13:08:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/03 13:08:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"My Spark Application\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01556bb0-76d9-41a3-8586-e6275b034fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, dayofweek,to_date, month, count, avg\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number,   sum, when\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path_flightdelay = \"./full_data_flightdelay.csv\"  # Replace with the path to your CSV file\n",
    "\n",
    "\n",
    "df_flightdelay = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(csv_file_path_flightdelay)\n",
    "\n",
    "\n",
    "# Read the CSV file using the manually defined schema\n",
    "csv_file_path_weather = \"./airport_weather_2019.csv\"  # Replace with your file path\n",
    "df_weather = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(csv_file_path_weather)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab71331e-dd5a-4fe4-96d8-971c711c5836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38675\n"
     ]
    }
   ],
   "source": [
    "print(df_weather.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c09dbf2-39ce-42c3-9e35-35ffc9c47fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/03 13:08:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+--------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "|MONTH|                NAME|              AWND|                PRCP|               SNOW|               SNWD|              TAVG|              TMAX|              TMIN|              WDF2|\n",
      "+-----+--------------------+------------------+--------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "|    1|ALBANY INTERNATIO...| 9.604516129032257| 0.13838709677419353| 0.6096774193548388| 2.5064516129032257|23.967741935483872| 31.93548387096774|14.709677419354838|231.29032258064515|\n",
      "|    2|ALBANY INTERNATIO...| 8.803214285714287| 0.09571428571428572| 0.5035714285714284|              2.525|27.857142857142858|35.392857142857146|             19.75|233.57142857142858|\n",
      "|    3|ALBANY INTERNATIO...| 9.698064516129032|0.045000000000000005|0.21290322580645163|  1.348387096774194|35.354838709677416| 44.58064516129032| 25.70967741935484|248.06451612903226|\n",
      "|    4|ALBANY INTERNATIO...| 9.573666666666668|               0.144|               0.07|               0.04|50.166666666666664| 59.93333333333333| 39.63333333333333|             234.0|\n",
      "|    5|ALBANY INTERNATIO...| 7.662903225806452|  0.0993548387096774|                0.0|                0.0|58.096774193548384| 67.93548387096774| 49.12903225806452|210.96774193548387|\n",
      "|    6|ALBANY INTERNATIO...| 6.658999999999998| 0.17166666666666666|                0.0|                0.0| 68.53333333333333| 78.93333333333334| 57.06666666666667|229.66666666666666|\n",
      "|    7|ALBANY INTERNATIO...|5.3835483870967735| 0.12870967741935482|                0.0|                0.0| 75.96774193548387|  86.6774193548387|  65.6774193548387|211.93548387096774|\n",
      "|    8|ALBANY INTERNATIO...| 5.613870967741936|  0.1329032258064516|                0.0|                0.0|  71.2258064516129|  82.2258064516129| 61.16129032258065| 216.1290322580645|\n",
      "|    9|ALBANY INTERNATIO...| 5.965333333333336| 0.07433333333333333|                0.0|                0.0| 64.06666666666666|              75.2|              52.4|197.66666666666666|\n",
      "|   10|ALBANY INTERNATIO...| 6.400967741935484| 0.24129032258064514|                0.0|                0.0|53.774193548387096| 62.74193548387097| 44.41935483870968|212.58064516129033|\n",
      "|   11|ALBANY INTERNATIO...| 8.060333333333332|               0.094|0.07666666666666667|               0.16| 37.13333333333333|44.733333333333334|              28.7|214.33333333333334|\n",
      "|   12|ALBANY INTERNATIO...| 7.137096774193547| 0.14645161290322578| 0.9000000000000001|  3.761290322580645|30.483870967741936|36.903225806451616|22.677419354838708|225.16129032258064|\n",
      "|    1|ALBUQUERQUE INTER...| 6.725161290322581|0.016774193548387096|0.06451612903225806|0.12903225806451613| 35.32258064516129| 46.03225806451613|25.806451612903224|  257.741935483871|\n",
      "|    2|ALBUQUERQUE INTER...| 9.251428571428573|0.017857142857142856|               0.15|0.11071428571428572|39.285714285714285|              51.5|28.321428571428573|239.64285714285714|\n",
      "|    3|ALBUQUERQUE INTER...| 10.17451612903226|0.023548387096774197|                0.0|                0.0| 50.29032258064516| 62.16129032258065|38.225806451612904|217.09677419354838|\n",
      "|    4|ALBUQUERQUE INTER...| 8.835666666666665|0.033666666666666664|                0.0|                0.0|              57.6| 70.26666666666667| 44.43333333333333|             215.0|\n",
      "|    5|ALBUQUERQUE INTER...|  9.77741935483871|0.007741935483870969|                0.0|                0.0| 61.70967741935484|              75.0|48.193548387096776|217.74193548387098|\n",
      "|    6|ALBUQUERQUE INTER...| 9.857333333333333|0.002333333333333...|                0.0|                0.0|              74.4|              87.9|              59.7|205.66666666666666|\n",
      "|    7|ALBUQUERQUE INTER...|  8.42032258064516| 0.06290322580645161|                0.0|                0.0| 80.03225806451613| 93.48387096774194| 67.90322580645162| 163.2258064516129|\n",
      "|    8|ALBUQUERQUE INTER...|7.2377419354838715|0.013870967741935483|                0.0|                0.0| 78.96774193548387|  93.2258064516129| 66.03225806451613|192.25806451612902|\n",
      "+-----+--------------------+------------------+--------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# create new column for month and day_of_week values derived from date\n",
    "df_day_column = df_weather.withColumn(\"DATE_NEW\", to_date(col(\"DATE\"), \"M/d/yyyy\"))\n",
    "df_day_column = df_day_column.withColumn(\"DATE_NEW\", coalesce(df_day_column[\"DATE_NEW\"], to_date(df_day_column[\"DATE\"], 'yyyy-MM-dd')))\n",
    "    \n",
    "df_day_column = df_day_column.withColumn(\"DAY_OF_WEEK\", dayofweek(col(\"DATE_NEW\").alias(\"DAY_OF_WEEK\")))\n",
    "df_day_column = df_day_column.withColumn(\"MONTH\", month(col(\"DATE_NEW\").alias(\"MONTH\")))\n",
    "#df_day_column = df_weather.withColumn(\"DAY_OF_WEEK\", dayofweek(col(\"DATE\").alias(\"DAY_OF_WEEK\"))) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df_day_column.show(n=2)\n",
    "df_day_column.createOrReplaceTempView(\"table1\")\n",
    "df_select = spark.sql(\"SELECT STATION, NAME,DAY_OF_WEEK,DATE, MONTH, AWND, PRCP, SNOW, SNWD, TAVG, TMAX, TMIN, WDF2 from table1\")\n",
    "#df_select.show(n=5)\n",
    "\n",
    "grouped_df = df_select.groupBy(\"MONTH\", \"NAME\").agg(\n",
    "    avg(\"AWND\").alias(\"AWND\"),\n",
    "    avg(\"PRCP\").alias(\"PRCP\"),\n",
    "    avg(\"SNOW\").alias(\"SNOW\"),\n",
    "    avg(\"SNWD\").alias(\"SNWD\"),\n",
    "    avg(\"TAVG\").alias(\"TAVG\"),\n",
    "    avg(\"TMAX\").alias(\"TMAX\"),\n",
    "    avg(\"TMIN\").alias(\"TMIN\"),\n",
    "    avg(\"WDF2\").alias(\"WDF2\")\n",
    ").orderBy(\"NAME\",\"MONTH\")\n",
    "\n",
    "\n",
    "grouped_df.show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "983d5e23-d23d-49cc-b55b-1427c1e118ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, split, col, lit, monotonically_increasing_id\n",
    "\n",
    "\n",
    "# Normalize joining columns\n",
    "grouped_df = grouped_df.withColumn(\"normalized_name\", lower(col(\"name\")))\n",
    "df_flightdelay = df_flightdelay.withColumn(\"normalized_name\", lower(split(col(\"departing_airport\"), \" \").getItem(0)))\n",
    "\n",
    "# Group by to investigate\n",
    "grouped_df_nn = grouped_df.groupBy(\"normalized_name\").agg(\n",
    "    count('*').alias('count')\n",
    ")\n",
    "\n",
    "grouped_df_name = grouped_df.groupBy(\"NAME\").agg(\n",
    "    count('*').alias('count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "706e59e6-c06d-41dd-838b-7bcd7c49b4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique __departing airports__ in flightdelay data:  86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEPARTING_AIRPORT|\n",
      "+-----------------+\n",
      "|  Eppley Airfield|\n",
      "|  Kahului Airport|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Only unique values\n",
    "grouped_df = grouped_df.select('name').distinct()\n",
    "#print(grouped_df.count())\n",
    "#print(\"Unique airports __names__ in flightdelay data: \",df_flightdelay.select('departing_airport').distinct().count())\n",
    "print(\"Unique __departing airports__ in flightdelay data: \",df_flightdelay.select('DEPARTING_AIRPORT').distinct().count())\n",
    "df_flightdelay.select('DEPARTING_AIRPORT').distinct().show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66837d93-05eb-4042-bf6b-1159fcb003df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|   departing_airport|                name|\n",
      "+--------------------+--------------------+\n",
      "|memphis internati...|memphis internati...|\n",
      "|portland internat...|portland internat...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Join dataframes grouped_df and df_flightdelay\n",
    "\n",
    "#joined_df = df_flightdelay.alias('f').join(\n",
    "#    grouped_df.alias('g'),\n",
    "#    (col('g.month') == col('f.month')) & (col('g.normalized_name')).contains(col('f.normalized_name')), 'inner'\n",
    "#)\n",
    "\n",
    "\n",
    "# For 'grouped_df', transforming 'NAME' to lowercase and dropping duplicates based on the 'name' column\n",
    "grouped_df_lower = grouped_df.select(lower(col(\"NAME\")).alias(\"name\")).dropDuplicates(['name'])\n",
    "\n",
    "# For 'df_flightdelay', transforming 'DEPARTING_AIRPORT' to lowercase, casting it to string, and dropping duplicates based on the 'departing_airport' column\n",
    "df_flightdelay_lower = df_flightdelay.select(lower(col(\"DEPARTING_AIRPORT\")).alias(\"departing_airport\")).dropDuplicates(['departing_airport'])\n",
    "\n",
    "\n",
    "\n",
    "#join providing table that contain in the name column all distinct airports \n",
    "#from weather dataset and under departing_flight all distinc airports from delay dataset\n",
    "print(grouped_df_lower.count())\n",
    "print(df_flightdelay_lower.count())\n",
    "result_df = df_flightdelay_lower.alias(\"flight\").join(\n",
    "    grouped_df_lower.alias(\"grouped\"),\n",
    "    (col(\"grouped.name\").contains(col(\"flight.departing_airport\"))),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"flight.departing_airport\").alias(\"departing_airport\"),\n",
    "    col(\"grouped.name\").alias(\"name\")\n",
    ")\n",
    "\n",
    "result_df.show(n=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cad8c016-135e-4c98-83d9-f36c28a72c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- departing_airport: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- departing_airport: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                name|   departing_airport|\n",
      "+--------------------+--------------------+\n",
      "|albany internatio...|tucson international|\n",
      "|albuquerque inter...|charleston intern...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#modify dataframe such that df_result will contain the airports matched on join \n",
    "#and enhanced results will contain the df of unmatched airports for each dataset\n",
    "\n",
    "# Identifying non-matched entries\n",
    "non_matched_flight = df_flightdelay_lower.alias(\"flight\").join(\n",
    "    result_df.alias(\"result\"),\n",
    "    result_df.departing_airport == df_flightdelay_lower.departing_airport,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "non_matched_grouped = grouped_df_lower.alias(\"grouped\").join(\n",
    "    result_df.alias(\"result\"),\n",
    "    result_df.name == grouped_df_lower.name,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "\n",
    "# Add a unique ID to each DataFrame to facilitate the outer join\n",
    "result_df = result_df.withColumn(\"id\", monotonically_increasing_id())\n",
    "non_matched_flight = non_matched_flight.withColumn(\"id\", monotonically_increasing_id())\n",
    "non_matched_grouped = non_matched_grouped.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "# Perform the outer joins using the unique IDs, result_df is now composed of matched airports\n",
    "enhanced_result_df = result_df.join(non_matched_flight, \"id\", \"outer\" ).join(non_matched_grouped, \"id\", \"outer\" )\n",
    "enhanced_result_df = enhanced_result_df.drop(\"id\")\n",
    "\n",
    "# Show the enhanced DataFrame with additional columns\n",
    "enhanced_result_df.printSchema()\n",
    "\n",
    "# Select columns, get rid of duplicates\n",
    "selected_columns = [col for col in enhanced_result_df.columns if col != 'name' and col != 'departing_airport'] + ['grouped.name'] + ['flight.departing_airport']\n",
    "\n",
    "#will contain unmatched airports for each dataset\n",
    "enhanced_result_df = enhanced_result_df.select(selected_columns)\n",
    "enhanced_result_df.drop('name','departing_airport')\n",
    "enhanced_result_df.show(n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a4742-f8c8-4707-97fe-808408ed0b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7419b18-8499-4eb5-b65b-3b70dda46365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---+--------------------+--------------------+\n",
      "|       delay_matched|     weather_matched| id|   weather_unmatched|     delay_unmatched|\n",
      "+--------------------+--------------------+---+--------------------+--------------------+\n",
      "|memphis internati...|memphis internati...|  0|albany internatio...|tucson international|\n",
      "|portland internat...|portland internat...|  1|albuquerque inter...|charleston intern...|\n",
      "+--------------------+--------------------+---+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataframe that contains airports matched and unmatched reuslt from the join\n",
    "# Rename columns in result_df\n",
    "result_df = result_df.withColumnRenamed(\"name\", \"weather_matched\") \\\n",
    "                     .withColumnRenamed(\"departing_airport\", \"delay_matched\")\n",
    "\n",
    "# Rename columns in enhanced_result_df\n",
    "enhanced_result_df = enhanced_result_df.withColumnRenamed(\"name\", \"weather_unmatched\") \\\n",
    "                                       .withColumnRenamed(\"departing_airport\", \"delay_unmatched\")\n",
    "\n",
    "# Optional: If you need to ensure the rows are matched by order, add an index column to each DataFrame\n",
    "result_df = result_df.withColumn(\"index\", monotonically_increasing_id())\n",
    "enhanced_result_df = enhanced_result_df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Join DataFrames on the index column\n",
    "matched_and_unmatched_airports = result_df.join(\n",
    "    enhanced_result_df,\n",
    "    on=\"index\",\n",
    "    how=\"outer\"  # Use \"outer\" to include all rows from both DataFrames\n",
    ")\n",
    "\n",
    "# Drop the index column as it's no longer needed after joining\n",
    "matched_and_unmatched_airports = matched_and_unmatched_airports.drop(\"index\", 'name', 'departing_airport')\n",
    "\n",
    "# Show the resulting DataFrame structure\n",
    "matched_and_unmatched_airports.drop('id')\n",
    "matched_and_unmatched_airports.show(n = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e793e46b-e808-4fea-ad76-6a90cef070c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-null strings in the 'name' column: 30 30 76 56\n"
     ]
    }
   ],
   "source": [
    "#count the amount of airports for each clumn in the enhanced dataset dataframe \n",
    "\n",
    "non_null_name_count = result_df.filter(col(\"name\").isNotNull()).count()\n",
    "non_null_name_count1 = result_df.filter(col(\"departing_airport\").isNotNull()).count()\n",
    "non_null_name_count2 = enhanced_result_df.filter(col(\"weather_unmatched\").isNotNull()).count()\n",
    "non_null_name_count3 = enhanced_result_df.filter(col(\"delay_unmatched\").isNotNull()).count()\n",
    "print(\"Number of non-null strings in the 'name' column:\", non_null_name_count, non_null_name_count1,non_null_name_count2, non_null_name_count3)\n",
    "\n",
    "# Display the filtered DataFrame and print the counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf47076e-52b6-4634-8e03-35ced09409df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Airport and City\n",
      "0              Goroka Airport, Goroka, Papua New Guinea\n",
      "1              Madang Airport, Madang, Papua New Guinea\n",
      "2     Mount Hagen Kagamuga Airport, Mount Hagen, Pap...\n",
      "3              Nadzab Airport, Nadzab, Papua New Guinea\n",
      "4     Port Moresby Jacksons International Airport, P...\n",
      "...                                                 ...\n",
      "7693                Rogachyovo Air Base, Belaya, Russia\n",
      "7694            Ulan-Ude East Airport, Ulan Ude, Russia\n",
      "7695             Krechevitsy Air Base, Novgorod, Russia\n",
      "7696        Desierto de Atacama Airport, Copiapo, Chile\n",
      "7697             Melitopol Air Base, Melitopol, Ukraine\n",
      "\n",
      "[7698 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the parsed data\n",
    "data = []\n",
    "\n",
    "# Open the text file and parse it line by line\n",
    "with open('./airports.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line by comma to extract the needed parts\n",
    "        parts = line.split(',')\n",
    "        \n",
    "        # Check if the line has enough parts to avoid index errors\n",
    "        if len(parts) >= 4:\n",
    "            # Extract and clean the desired parts\n",
    "            # Remove quotation marks and extra spaces if present\n",
    "            name = parts[1].strip('\"').strip()\n",
    "            city = parts[2].strip('\"').strip()\n",
    "            country = parts[3].strip('\"').strip()\n",
    "            \n",
    "            # Combine the first two parts into one column, and keep the country as the second column\n",
    "            combined = f\"{name}, {city}, {country}\"\n",
    "            data.append(combined)\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df_airports = pd.DataFrame(data, columns=['Airport and City'])\n",
    "\n",
    "# Display the DataFrame to verify it's correct\n",
    "print(df_airports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8392322f-bd5d-47ea-ad10-763847e59700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fuzzywuzzy in /home/hel/.local/lib/python3.10/site-packages (0.18.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-Levenshtein in /home/hel/.local/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: Levenshtein==0.25.1 in /home/hel/.local/lib/python3.10/site-packages (from python-Levenshtein) (0.25.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in /home/hel/.local/lib/python3.10/site-packages (from Levenshtein==0.25.1->python-Levenshtein) (3.8.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    weather_unmatched  \\\n",
      "0   anchorage ted stevens international airport, a...   \n",
      "1       austin bergstrom international airport, tx us   \n",
      "2                                       boston, ma us   \n",
      "3                     charleston intl. airport, sc us   \n",
      "4      cleveland hopkins international airport, oh us   \n",
      "5                           dallas faa airport, tx us   \n",
      "6             des moines international airport, ia us   \n",
      "7        fort lauderdale international airport, fl us   \n",
      "8                           greensboro airport, nc us   \n",
      "9           indianapolis international airport, in us   \n",
      "10                           knoxville airport, tn us   \n",
      "11                  milwaukee mitchell airport, wi us   \n",
      "12  minneapolis st. paul international airport, mn us   \n",
      "13                         new orleans airport, la us   \n",
      "14                             phoenix airport, az us   \n",
      "15                             raleigh airport, nc us   \n",
      "16                                reno airport, nv us   \n",
      "17             san diego international airport, ca us   \n",
      "18                santa ana john wayne airport, ca us   \n",
      "19              savannah international airport, ga us   \n",
      "20                      seattle tacoma airport, wa us   \n",
      "21      st louis lambert international airport, mo us   \n",
      "\n",
      "                              delay_unmatched  \\\n",
      "0                     anchorage international   \n",
      "1            austin - bergstrom international   \n",
      "2                         logan international   \n",
      "3                    charleston international   \n",
      "4             cleveland-hopkins international   \n",
      "5                  dallas fort worth regional   \n",
      "6                        des moines municipal   \n",
      "7     fort lauderdale-hollywood international   \n",
      "8                piedmont triad international   \n",
      "9                 indianapolis muni/weir cook   \n",
      "10                               mcghee tyson   \n",
      "11                     general mitchell field   \n",
      "12          minneapolis-st paul international   \n",
      "13  louis armstrong new orleans international   \n",
      "14           phoenix sky harbor international   \n",
      "15               raleigh-durham international   \n",
      "16                   reno/tahoe international   \n",
      "17       san diego international lindbergh fl   \n",
      "18                              orange county   \n",
      "19         savannah/hilton head international   \n",
      "20                      seattle international   \n",
      "21            lambert-st. louis international   \n",
      "\n",
      "                               Airport and City_match  \n",
      "0   Ted Stevens Anchorage International Airport, A...  \n",
      "1   Austin Bergstrom International Airport, Austin...  \n",
      "2   General Edward Lawrence Logan International Ai...  \n",
      "3   Charleston Air Force Base-International Airpor...  \n",
      "4   Cleveland Hopkins International Airport, Cleve...  \n",
      "5   Dallas Fort Worth International Airport, Dalla...  \n",
      "6   Des Moines International Airport, Des Moines, ...  \n",
      "7   Fort Lauderdale Hollywood International Airpor...  \n",
      "8   Piedmont Triad International Airport, Greensbo...  \n",
      "9   Indianapolis International Airport, Indianapol...  \n",
      "10     McGhee Tyson Airport, Knoxville, United States  \n",
      "11  General Mitchell International Airport, Milwau...  \n",
      "12  Minneapolis-St Paul International/Wold-Chamber...  \n",
      "13  Louis Armstrong New Orleans International Airp...  \n",
      "14  Phoenix Sky Harbor International Airport, Phoe...  \n",
      "15  Raleigh Durham International Airport, Raleigh-...  \n",
      "16  Reno Tahoe International Airport, Reno, United...  \n",
      "17  San Diego International Airport, San Diego, Un...  \n",
      "18  John Wayne Airport-Orange County Airport, Sant...  \n",
      "19  Savannah Hilton Head International Airport, Sa...  \n",
      "20  Seattle Tacoma International Airport, Seattle,...  \n",
      "21  St Louis Lambert International Airport, St. Lo...  \n",
      "+--------------------+--------------------+----------------------+\n",
      "|   weather_unmatched|     delay_unmatched|Airport and City_match|\n",
      "+--------------------+--------------------+----------------------+\n",
      "|anchorage ted ste...|anchorage interna...|  Ted Stevens Ancho...|\n",
      "|austin bergstrom ...|austin - bergstro...|  Austin Bergstrom ...|\n",
      "|       boston, ma us| logan international|  General Edward La...|\n",
      "|charleston intl. ...|charleston intern...|  Charleston Air Fo...|\n",
      "|cleveland hopkins...|cleveland-hopkins...|  Cleveland Hopkins...|\n",
      "|dallas faa airpor...|dallas fort worth...|  Dallas Fort Worth...|\n",
      "|des moines intern...|des moines municipal|  Des Moines Intern...|\n",
      "|fort lauderdale i...|fort lauderdale-h...|  Fort Lauderdale H...|\n",
      "|greensboro airpor...|piedmont triad in...|  Piedmont Triad In...|\n",
      "|indianapolis inte...|indianapolis muni...|  Indianapolis Inte...|\n",
      "|knoxville airport...|        mcghee tyson|  McGhee Tyson Airp...|\n",
      "|milwaukee mitchel...|general mitchell ...|  General Mitchell ...|\n",
      "|minneapolis st. p...|minneapolis-st pa...|  Minneapolis-St Pa...|\n",
      "|new orleans airpo...|louis armstrong n...|  Louis Armstrong N...|\n",
      "|phoenix airport, ...|phoenix sky harbo...|  Phoenix Sky Harbo...|\n",
      "|raleigh airport, ...|raleigh-durham in...|  Raleigh Durham In...|\n",
      "| reno airport, nv us|reno/tahoe intern...|  Reno Tahoe Intern...|\n",
      "|san diego interna...|san diego interna...|  San Diego Interna...|\n",
      "|santa ana john wa...|       orange county|  John Wayne Airpor...|\n",
      "|savannah internat...|savannah/hilton h...|  Savannah Hilton H...|\n",
      "+--------------------+--------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein\n",
    "\n",
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "def get_matches(df1, col1, df2, col2, threshold=40):\n",
    "    # Convert each column to a list for processing, ensuring to drop NA values\n",
    "    list1 = df1[col1].dropna().tolist()\n",
    "    list2 = df2[col2].dropna().tolist()\n",
    "\n",
    "    # Find best matches with a score above the threshold\n",
    "    matches = []\n",
    "    for item in list1:\n",
    "        # Use process.extractOne to find the best match for each item from list1 in list2\n",
    "        best_match = process.extractOne(item, list2, scorer=fuzz.token_set_ratio)\n",
    "        if best_match and best_match[1] >= threshold:\n",
    "            matches.append((item, best_match[0], best_match[1]))\n",
    "\n",
    "    # Return matches as a DataFrame for better visualization\n",
    "    return pd.DataFrame(matches, columns=[col1, col2 + '_match', 'Score'])\n",
    "\n",
    "# Assuming 'matched_and_unmatched_airports' is your PySpark DataFrame\n",
    "pandas_df = matched_and_unmatched_airports.toPandas()  # Convert to Pandas DataFrame\n",
    "\n",
    "# Example usage (ensure df1 and df2 are already defined and loaded with your data)\n",
    "df_matches_weather = get_matches(pandas_df, 'weather_unmatched', df_airports, 'Airport and City')\n",
    "df_matches_delay = get_matches(pandas_df, 'delay_unmatched', df_airports, 'Airport and City')\n",
    "\n",
    "\n",
    "# Merge the two match DataFrames on the 'Airport and City' match column\n",
    "combined_matches = pd.merge(\n",
    "    df_matches_weather,\n",
    "    df_matches_delay,\n",
    "    on='Airport and City_match',\n",
    "    suffixes=('_weather', '_delay')\n",
    ")\n",
    "\n",
    "# Select the relevant columns\n",
    "final_matches = combined_matches[['weather_unmatched', 'delay_unmatched', 'Airport and City_match']]\n",
    "\n",
    "# Rename 'Airport and City_match' for clarity\n",
    "final_matches.rename(columns={'Airport and City_match': 'Airport and City'})\n",
    "\n",
    "\n",
    "print(final_matches)\n",
    "\n",
    "spark_df = spark.createDataFrame(final_matches)\n",
    "\n",
    "# Show the DataFrame to verify conversion\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7dc5e-1efd-4a5b-9d0c-cccea01c9bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joined_df.count())\n",
    "print(df_flightdelay.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f429f-79b7-4500-95e5-69adc835f84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fractions = {label: 0.1 for label in joined_df.select(\"DEP_DEL15\").distinct().rdd.flatMap(lambda x: x).collect()}\n",
    "sampled_df = joined_df.stat.sampleBy(\"DEP_DEL15\", fractions, seed=1234)\n",
    "\n",
    "# Show the sampled data distribution\n",
    "sampled_df.groupBy(\"DEP_DEL15\").count().show()\n",
    "\n",
    "# Split the DataFrame into training (60%) and test (40%) sets\n",
    "train_df, test_df = sampled_df.randomSplit([0.6, 0.4], seed=1234)\n",
    "\n",
    "# Show the size of each set\n",
    "print(\"Training Dataset Count: \" + str(train_df.count()))\n",
    "print(\"Testing Dataset Count: \" + str(test_df.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03660f-4c5e-4e00-a7fd-2e9c5457d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical to nominal\n",
    "# Calculate the quantile thresholds\n",
    "thresholds = joined_df.approxQuantile(\"PRCP\", [0.33, 0.67], 0.01)  # 0.01 is the relative error\n",
    "\n",
    "# Categorize based on quantile thresholds\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"precip_category\",\n",
    "    when(col(\"PRCP\") <= thresholds[0], \"low\")\n",
    "    .when(col(\"PRCP\") <= thresholds[1], \"medium\")\n",
    "    .otherwise(\"high\")\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "joined_df.select(\"PRCP\", \"precip_category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69caaf58-3343-4759-9f6b-3df108905834",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dfi_lower = grouped_df.select(col(\"NAME\").alias(\"name\").cast(\"string\"))\n",
    "\n",
    "# Select columns from df_flightdelay and rename to lowercase\n",
    "df_flightdelay_lower = df_flightdelay.select(col(\"DEPARTING_AIRPORT\").alias(\"departing_airport\").cast(\"string\"))\n",
    "\n",
    "# Join the two DataFrames on some common column, for example, index\n",
    "result_df = grouped_dfi_lower.join(df_flightdelay_lower, grouped_dfi_lower.index == df_flightdelay_lower.index, \"inner\").drop(df_flightdelay_lower.index)\n",
    "show(result_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
